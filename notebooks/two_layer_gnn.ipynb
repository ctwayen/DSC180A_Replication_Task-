{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import section\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_layer_GraphNet(nn.Module):\n",
    "    def __init__(self, A, device, F = 1433, class_number=7, hidden_neurons = 200):\n",
    "        super(two_layer_GraphNet, self).__init__()\n",
    "        # precompute adjacency matrix before training\n",
    "        if A[0][0] == 0:\n",
    "            A = A * 0.1 + np.identity(A.shape[0])\n",
    "        self.A = torch.from_numpy(A).float()\n",
    "        self.class_number = class_number\n",
    "        self.fc1 = nn.Linear(F, hidden_neurons, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_neurons, self.class_number, bias=True)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # training on full x, not batch\n",
    "        x = x.float()\n",
    "        # average all neighboors\n",
    "        #print(x.shape)\n",
    "        #A = self.A.float()\n",
    "        A = self.A.to(self.device)\n",
    "        #print(A.shape)\n",
    "        #print(self.X.shape)\n",
    "        #print(A.dtype, self.X.dtype, x.dtype)\n",
    "        x = torch.matmul(A, x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN():\n",
    "    def __init__(self, hidden_neurons=200, learning_rate=1e-3, epoch=50, device='cuda'):\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = epoch\n",
    "        self.device = torch.device(device)\n",
    "        self.kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    \n",
    "    def train(self, model, device, train_loader, optimizer):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            #print(output, target)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('training loss {:.4f}'.format(loss.item()))\n",
    "\n",
    "    def test(self, model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(test_loader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('Average loss: {:.4f}, Accuracy: {:.4f}%'.format(test_loss, 100. * correct / len(test_loader.dataset)))\n",
    "        return 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    def encode_label(self, y):\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        label_encoder = preprocessing.LabelEncoder()\n",
    "        label_encoder.fit(y)\n",
    "        y = label_encoder.transform(y)\n",
    "        return y\n",
    "    \n",
    "    def fit(self, X, y, A):\n",
    "        X = torch.from_numpy(X).type(torch.long)\n",
    "        self.A = A\n",
    "        y = torch.from_numpy(self.encode_label(y))\n",
    "        self.dataset = TensorDataset(X, y)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=2708, shuffle=True, **self.kwargs)\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        model = two_layer_GraphNet(self.A, self.device)\n",
    "        model.to(self.device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "        accs = {'acc': []}\n",
    "        for epoch in range(self.epoch):\n",
    "            self.train(model, self.device, self.dataloader , optimizer)\n",
    "            acc = self.test(model, self.device, self.dataloader)\n",
    "            accs['acc'].append(acc)\n",
    "        return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = GNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coraloader import cora_loader\n",
    "cora = cora_loader('data/cora.content', 'data/cora.cites', None)\n",
    "X, y, A = cora.get_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn.fit(X, y, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 1.9513\n",
      "Average loss: 1.9424, Accuracy: 15.2511%\n",
      "training loss 1.9425\n",
      "Average loss: 1.9342, Accuracy: 29.9852%\n",
      "training loss 1.9342\n",
      "Average loss: 1.9237, Accuracy: 38.7740%\n",
      "training loss 1.9237\n",
      "Average loss: 1.9089, Accuracy: 47.4520%\n",
      "training loss 1.9091\n",
      "Average loss: 1.8904, Accuracy: 57.2747%\n",
      "training loss 1.8904\n",
      "Average loss: 1.8699, Accuracy: 61.7799%\n",
      "training loss 1.8684\n",
      "Average loss: 1.8466, Accuracy: 60.1182%\n",
      "training loss 1.8454\n",
      "Average loss: 1.8214, Accuracy: 58.8996%\n",
      "training loss 1.8211\n",
      "Average loss: 1.7955, Accuracy: 55.6869%\n",
      "training loss 1.7935\n",
      "Average loss: 1.7634, Accuracy: 52.9542%\n",
      "training loss 1.7656\n",
      "Average loss: 1.7362, Accuracy: 50.8124%\n",
      "training loss 1.7367\n",
      "Average loss: 1.7041, Accuracy: 49.3722%\n",
      "training loss 1.7059\n",
      "Average loss: 1.6703, Accuracy: 49.6307%\n",
      "training loss 1.6697\n",
      "Average loss: 1.6385, Accuracy: 50.0000%\n",
      "training loss 1.6392\n",
      "Average loss: 1.6051, Accuracy: 49.6307%\n",
      "training loss 1.6002\n",
      "Average loss: 1.5661, Accuracy: 50.7016%\n",
      "training loss 1.5648\n",
      "Average loss: 1.5291, Accuracy: 52.0679%\n",
      "training loss 1.5264\n",
      "Average loss: 1.4909, Accuracy: 54.6160%\n",
      "training loss 1.4954\n",
      "Average loss: 1.4518, Accuracy: 56.7208%\n",
      "training loss 1.4573\n",
      "Average loss: 1.4111, Accuracy: 59.4165%\n",
      "training loss 1.4198\n",
      "Average loss: 1.3733, Accuracy: 64.0325%\n",
      "training loss 1.3735\n",
      "Average loss: 1.3324, Accuracy: 67.4668%\n",
      "training loss 1.3363\n",
      "Average loss: 1.2957, Accuracy: 70.2733%\n",
      "training loss 1.2914\n",
      "Average loss: 1.2603, Accuracy: 72.4889%\n",
      "training loss 1.2579\n",
      "Average loss: 1.2203, Accuracy: 75.6278%\n",
      "training loss 1.2152\n",
      "Average loss: 1.1788, Accuracy: 77.6957%\n",
      "training loss 1.1808\n",
      "Average loss: 1.1395, Accuracy: 79.1728%\n",
      "training loss 1.1405\n",
      "Average loss: 1.1021, Accuracy: 80.6130%\n",
      "training loss 1.1044\n",
      "Average loss: 1.0612, Accuracy: 80.6130%\n",
      "training loss 1.0634\n",
      "Average loss: 1.0309, Accuracy: 81.9055%\n",
      "training loss 1.0261\n",
      "Average loss: 0.9871, Accuracy: 81.9793%\n",
      "training loss 0.9856\n",
      "Average loss: 0.9573, Accuracy: 82.7548%\n",
      "training loss 0.9537\n",
      "Average loss: 0.9220, Accuracy: 83.1610%\n",
      "training loss 0.9214\n",
      "Average loss: 0.8893, Accuracy: 83.5303%\n",
      "training loss 0.8865\n",
      "Average loss: 0.8575, Accuracy: 84.1581%\n",
      "training loss 0.8573\n",
      "Average loss: 0.8213, Accuracy: 84.1581%\n",
      "training loss 0.8302\n",
      "Average loss: 0.7996, Accuracy: 84.5273%\n",
      "training loss 0.7902\n",
      "Average loss: 0.7697, Accuracy: 85.0812%\n",
      "training loss 0.7645\n",
      "Average loss: 0.7355, Accuracy: 85.7829%\n",
      "training loss 0.7404\n",
      "Average loss: 0.7041, Accuracy: 86.7430%\n",
      "training loss 0.7137\n",
      "Average loss: 0.6909, Accuracy: 86.3368%\n",
      "training loss 0.6884\n",
      "Average loss: 0.6617, Accuracy: 86.8168%\n",
      "training loss 0.6623\n",
      "Average loss: 0.6369, Accuracy: 87.0015%\n",
      "training loss 0.6393\n",
      "Average loss: 0.6248, Accuracy: 87.0384%\n",
      "training loss 0.6224\n",
      "Average loss: 0.5953, Accuracy: 87.7031%\n",
      "training loss 0.5968\n",
      "Average loss: 0.5666, Accuracy: 88.6263%\n",
      "training loss 0.5781\n",
      "Average loss: 0.5585, Accuracy: 88.1093%\n",
      "training loss 0.5536\n",
      "Average loss: 0.5354, Accuracy: 88.7001%\n",
      "training loss 0.5401\n",
      "Average loss: 0.5226, Accuracy: 89.3279%\n",
      "training loss 0.5177\n",
      "Average loss: 0.5070, Accuracy: 89.1064%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': [15.251107828655835,\n",
       "  29.98522895125554,\n",
       "  38.77400295420975,\n",
       "  47.4519940915805,\n",
       "  57.27474150664697,\n",
       "  61.779911373707534,\n",
       "  60.118168389955684,\n",
       "  58.899556868537665,\n",
       "  55.68685376661743,\n",
       "  52.95420974889217,\n",
       "  50.81240768094535,\n",
       "  49.37223042836041,\n",
       "  49.630723781388475,\n",
       "  50.0,\n",
       "  49.630723781388475,\n",
       "  50.70162481536189,\n",
       "  52.06794682422452,\n",
       "  54.61595273264402,\n",
       "  56.72082717872969,\n",
       "  59.416543574593796,\n",
       "  64.0324963072378,\n",
       "  67.46676514032497,\n",
       "  70.27326440177252,\n",
       "  72.48892171344166,\n",
       "  75.62776957163959,\n",
       "  77.69571639586411,\n",
       "  79.1728212703102,\n",
       "  80.61299852289513,\n",
       "  80.61299852289513,\n",
       "  81.90546528803544,\n",
       "  81.97932053175775,\n",
       "  82.75480059084195,\n",
       "  83.16100443131462,\n",
       "  83.53028064992614,\n",
       "  84.15805022156573,\n",
       "  84.15805022156573,\n",
       "  84.52732644017725,\n",
       "  85.08124076809453,\n",
       "  85.78286558345643,\n",
       "  86.74298375184638,\n",
       "  86.33677991137371,\n",
       "  86.81683899556869,\n",
       "  87.00147710487444,\n",
       "  87.03840472673559,\n",
       "  87.70310192023634,\n",
       "  88.62629246676514,\n",
       "  88.109305760709,\n",
       "  88.70014771048744,\n",
       "  89.32791728212703,\n",
       "  89.10635155096011]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn.train_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
