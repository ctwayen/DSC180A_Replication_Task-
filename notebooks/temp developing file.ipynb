{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test area\n",
    "#x = np.array([[1, 1, 3], [2, 2, 3]])\n",
    "#x = x[:, [i for i in range(x.shape[1]) if i not in [0, 1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def readF(file_path, delimiter='\\t', header=None, y_index=-1, ignore_columns=None):\n",
    "    '''\n",
    "    @ Params\n",
    "        file_path: a string indicate the path of the file\n",
    "        delimiter: the delimiter for delimite the file, default is '\\t'\n",
    "        header: default is none\n",
    "        y_index: the index of labels, default is the last one\n",
    "        ignore_columns: a list of indexs representing correponding columns that need to be ignored\n",
    "\n",
    "    @ Output\n",
    "        X and y in terms of numpy array\n",
    "    '''\n",
    "    features = pd.read_csv(file_path, header=header, delimiter=delimiter).to_numpy()\n",
    "    if ignore_columns != None:\n",
    "        features = features[:, [x for x in range(features.shape[1]) if x not in ignore_columns]]\n",
    "    if y_index == -1:\n",
    "        y_index = features.shape[1] - 1\n",
    "    X = features[:, [x for x in range(features.shape[1]) if x != y_index]]\n",
    "    y = features[:, y_index]\n",
    "    return X.astype('int64'), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readedges(file_path, adj_matrix=False):\n",
    "    '''\n",
    "    @ params\n",
    "        file_path: a string indicate the path of the file\n",
    "        adj_matrix: wether return the graph as adj matrix, default False\n",
    "    '''\n",
    "    G = nx.read_edgelist(file_path)\n",
    "    if adj_matrix:\n",
    "        return nx.adjacency_matrix(G).todense()\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(y):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(y)\n",
    "    labels = to_categorical(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/graph-convolutional-networks-on-node-classification-2b6bbec1d042\n",
    "class cora_loader():\n",
    "    def __init__(self, path_con, path_city):\n",
    "        '''\n",
    "            @ paramsï¼š\n",
    "                path_con: path for the file cora.content\n",
    "                path_city: path for the file cora.cities\n",
    "                ratio: a list represent [ratio of training, ratio of test]\n",
    "        '''\n",
    "        all_data = []\n",
    "        all_edges = []\n",
    "        with open(path_con) as f:\n",
    "            all_data.extend(f.read().splitlines())\n",
    "        with open(path_city) as f:\n",
    "            all_edges.extend(f.read().splitlines())\n",
    "        random.seed(4)\n",
    "        random.shuffle(all_data)\n",
    "        labels = []\n",
    "        nodes = []\n",
    "        X = []\n",
    "\n",
    "        for i , data in enumerate(all_data):\n",
    "            elements = data.split('\\t')\n",
    "            labels.append(elements[-1])\n",
    "            X.append(elements[1:-1])\n",
    "            nodes.append(elements[0])\n",
    "\n",
    "        self.X = np.array(X,dtype=int)\n",
    "        self.y = labels\n",
    "        #parse the edge\n",
    "        edge_list=[]\n",
    "        for edge in all_edges:\n",
    "            e = edge.split('\\t')\n",
    "            edge_list.append((e[0],e[1]))\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(nodes)\n",
    "        G.add_edges_from(edge_list)\n",
    "\n",
    "        #obtain the adjacency matrix (A)\n",
    "        self.A = nx.adjacency_matrix(G)\n",
    "    def get_train(self):\n",
    "        return self.X, self.y, self.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, Callback\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from spektral.layers import GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gnn_nlayer():\n",
    "    def __init__ (self, channels = 16, dropout=0.1, l2_reg = 5e-4):\n",
    "        '''\n",
    "            channels: output channels of each layer\n",
    "            dropout: dropout ratio of dropout layer\n",
    "            l2_reg: if -1, then no reularization\n",
    "        '''\n",
    "        self.channels = channels\n",
    "        self.drop_out = dropout\n",
    "        self.l2_reg = l2_reg\n",
    "        \n",
    "    def fit(self, X, y, A, num_classes = 7, train_part = 0.5):\n",
    "        '''\n",
    "            X: features\n",
    "            A: adj matrix\n",
    "        '''\n",
    "        self.A = GraphConv.preprocess(A).astype('f4')\n",
    "        self.F = X.shape[1]\n",
    "        self.N = X.shape[0]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        #https://towardsdatascience.com/graph-convolutional-networks-on-node-classification-2b6bbec1d042\n",
    "        train_idx = np.random.choice(len(self.y), int(np.ceil(train_part * len(self.y))), replace=False)\n",
    "\n",
    "        #get the indices that do not go to traning data\n",
    "        val_idx = [x for x in range(len(self.y)) if x not in train_idx]\n",
    "        train_mask = np.zeros((self.N,),dtype=bool)\n",
    "        train_mask[train_idx] = True\n",
    "        self.train_mask = train_mask\n",
    "        val_mask = np.zeros((self.N,),dtype=bool)\n",
    "        val_mask[val_idx] = True\n",
    "        self.val_mask = val_mask\n",
    "        self.y = encode_label(self.y)\n",
    "        \n",
    "    def model(self, n):\n",
    "        input_layer = Input(shape=(self.F, ))\n",
    "        filter_in = Input((self.N, ), sparse=True)\n",
    "        dropout = Dropout(self.drop_out)(input_layer)\n",
    "        graph_conv_ = GraphConv(self.channels,\n",
    "                             activation='relu',\n",
    "                             use_bias=True,\n",
    "                             kernel_regularizer=l2(self.l2_reg))([dropout, filter_in])\n",
    "        for i in range(1, n):\n",
    "            dropout = Dropout(self.drop_out)(graph_conv_)\n",
    "            graph_conv_ = GraphConv(\n",
    "                                     self.channels,\n",
    "                                     kernel_regularizer=l2(self.l2_reg),\n",
    "                                     use_bias=True,\n",
    "                                     activation='relu')([dropout, filter_in]\n",
    "                                    )\n",
    "\n",
    "        dropout_ = Dropout(self.drop_out)(graph_conv_)\n",
    "        graph_conv_ = GraphConv(self.num_classes,\n",
    "                                 use_bias=True,\n",
    "                                 activation='softmax')([dropout_, filter_in])\n",
    "        model = Model(inputs=[input_layer, filter_in], outputs=graph_conv_)\n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def mcompile(self, optimizer='adam', loss='cr', learning_rate=1e-3):\n",
    "        if optimizer == 'adam':\n",
    "            optimizer = Adam(lr=learning_rate)\n",
    "        if loss == 'cr':\n",
    "            loss = 'categorical_crossentropy'\n",
    "        self.model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  weighted_metrics=['acc'])\n",
    "        display(self.model.summary())\n",
    "        return self.model\n",
    "    \n",
    "    def train(self, epochs=50):\n",
    "        validation_data = ([self.X, self.A], self.y, self.val_mask)\n",
    "        hist = self.model.fit([self.X, self.A],\n",
    "                  self.y,\n",
    "                  sample_weight=self.train_mask,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=self.N,\n",
    "                  validation_data=validation_data,\n",
    "                  shuffle=False)\n",
    "        return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = cora_loader('data/cora.content', 'data/cora.cites')\n",
    "model_2 = gnn_nlayer()\n",
    "X, y, A = cora.get_train()\n",
    "model_2.fit(X, y, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7fc6307f2bd0>"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_35\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_41 (InputLayer)           [(None, 1433)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 1433)         0           input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_42 (InputLayer)           [(None, 2708)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_conv_55 (GraphConv)       (None, 16)           22944       dropout_56[0][0]                 \n",
      "                                                                 input_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 16)           0           graph_conv_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "graph_conv_56 (GraphConv)       (None, 16)           272         dropout_57[0][0]                 \n",
      "                                                                 input_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 16)           0           graph_conv_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "graph_conv_57 (GraphConv)       (None, 7)            119         dropout_58[0][0]                 \n",
      "                                                                 input_42[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,335\n",
      "Trainable params: 23,335\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7fc6307f2bd0>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.mcompile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "1/1 [==============================] - 1s 743ms/step - loss: 0.9981 - acc: 0.1263 - val_loss: 0.9942 - val_acc: 0.1736\n",
      "Epoch 2/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.9944 - acc: 0.1595 - val_loss: 0.9914 - val_acc: 0.2341\n",
      "Epoch 3/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.9915 - acc: 0.2334 - val_loss: 0.9888 - val_acc: 0.2925\n",
      "Epoch 4/250\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.9887 - acc: 0.2629 - val_loss: 0.9861 - val_acc: 0.3250\n",
      "Epoch 5/250\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9849 - acc: 0.3560 - val_loss: 0.9834 - val_acc: 0.3442\n",
      "Epoch 6/250\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9832 - acc: 0.3501 - val_loss: 0.9806 - val_acc: 0.3575\n",
      "Epoch 7/250\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9796 - acc: 0.3796 - val_loss: 0.9777 - val_acc: 0.3641\n",
      "Epoch 8/250\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.9758 - acc: 0.4025 - val_loss: 0.9747 - val_acc: 0.3722\n",
      "Epoch 9/250\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.9732 - acc: 0.3988 - val_loss: 0.9716 - val_acc: 0.3848\n",
      "Epoch 10/250\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.9696 - acc: 0.4092 - val_loss: 0.9683 - val_acc: 0.3936\n",
      "Epoch 11/250\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.9668 - acc: 0.4106 - val_loss: 0.9649 - val_acc: 0.4003\n",
      "Epoch 12/250\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.9621 - acc: 0.4306 - val_loss: 0.9614 - val_acc: 0.4040\n",
      "Epoch 13/250\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.9589 - acc: 0.4328 - val_loss: 0.9577 - val_acc: 0.4129\n",
      "Epoch 14/250\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.9552 - acc: 0.4586 - val_loss: 0.9538 - val_acc: 0.4269\n",
      "Epoch 15/250\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.9519 - acc: 0.4690 - val_loss: 0.9499 - val_acc: 0.4350\n",
      "Epoch 16/250\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.9461 - acc: 0.5037 - val_loss: 0.9458 - val_acc: 0.4520\n",
      "Epoch 17/250\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.9421 - acc: 0.5081 - val_loss: 0.9417 - val_acc: 0.4719\n",
      "Epoch 18/250\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.9367 - acc: 0.5037 - val_loss: 0.9374 - val_acc: 0.4793\n",
      "Epoch 19/250\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.9310 - acc: 0.5222 - val_loss: 0.9329 - val_acc: 0.4897\n",
      "Epoch 20/250\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.9274 - acc: 0.5569 - val_loss: 0.9284 - val_acc: 0.4956\n",
      "Epoch 21/250\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.9232 - acc: 0.5443 - val_loss: 0.9237 - val_acc: 0.5052\n",
      "Epoch 22/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.9198 - acc: 0.5620 - val_loss: 0.9189 - val_acc: 0.5155\n",
      "Epoch 23/250\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.9134 - acc: 0.5443 - val_loss: 0.9140 - val_acc: 0.5199\n",
      "Epoch 24/250\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.9065 - acc: 0.5628 - val_loss: 0.9089 - val_acc: 0.5177\n",
      "Epoch 25/250\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.9026 - acc: 0.5768 - val_loss: 0.9037 - val_acc: 0.5155\n",
      "Epoch 26/250\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8959 - acc: 0.5702 - val_loss: 0.8984 - val_acc: 0.5148\n",
      "Epoch 27/250\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.8924 - acc: 0.5554 - val_loss: 0.8929 - val_acc: 0.5185\n",
      "Epoch 28/250\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8848 - acc: 0.5451 - val_loss: 0.8874 - val_acc: 0.5207\n",
      "Epoch 29/250\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.8811 - acc: 0.5591 - val_loss: 0.8818 - val_acc: 0.5244\n",
      "Epoch 30/250\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.8734 - acc: 0.5665 - val_loss: 0.8761 - val_acc: 0.5295\n",
      "Epoch 31/250\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.8680 - acc: 0.5547 - val_loss: 0.8704 - val_acc: 0.5281\n",
      "Epoch 32/250\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.8622 - acc: 0.5805 - val_loss: 0.8646 - val_acc: 0.5303\n",
      "Epoch 33/250\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8559 - acc: 0.5679 - val_loss: 0.8587 - val_acc: 0.5303\n",
      "Epoch 34/250\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.8493 - acc: 0.5923 - val_loss: 0.8527 - val_acc: 0.5325\n",
      "Epoch 35/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8430 - acc: 0.5716 - val_loss: 0.8466 - val_acc: 0.5310\n",
      "Epoch 36/250\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.8365 - acc: 0.5820 - val_loss: 0.8405 - val_acc: 0.5347\n",
      "Epoch 37/250\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.8303 - acc: 0.5835 - val_loss: 0.8343 - val_acc: 0.5451\n",
      "Epoch 38/250\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.8230 - acc: 0.5908 - val_loss: 0.8281 - val_acc: 0.5539\n",
      "Epoch 39/250\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.8128 - acc: 0.5894 - val_loss: 0.8217 - val_acc: 0.5554\n",
      "Epoch 40/250\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.8073 - acc: 0.5945 - val_loss: 0.8153 - val_acc: 0.5606\n",
      "Epoch 41/250\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8078 - acc: 0.5849 - val_loss: 0.8089 - val_acc: 0.5672\n",
      "Epoch 42/250\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.7961 - acc: 0.5953 - val_loss: 0.8024 - val_acc: 0.5775\n",
      "Epoch 43/250\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7902 - acc: 0.6064 - val_loss: 0.7959 - val_acc: 0.5835\n",
      "Epoch 44/250\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.7815 - acc: 0.6226 - val_loss: 0.7894 - val_acc: 0.5931\n",
      "Epoch 45/250\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7767 - acc: 0.6064 - val_loss: 0.7828 - val_acc: 0.6004\n",
      "Epoch 46/250\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7686 - acc: 0.6344 - val_loss: 0.7762 - val_acc: 0.6086\n",
      "Epoch 47/250\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7622 - acc: 0.6359 - val_loss: 0.7696 - val_acc: 0.6182\n",
      "Epoch 48/250\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7535 - acc: 0.6640 - val_loss: 0.7630 - val_acc: 0.6211\n",
      "Epoch 49/250\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.7455 - acc: 0.6411 - val_loss: 0.7563 - val_acc: 0.6285\n",
      "Epoch 50/250\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7394 - acc: 0.6551 - val_loss: 0.7497 - val_acc: 0.6359\n",
      "Epoch 51/250\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.7322 - acc: 0.6647 - val_loss: 0.7430 - val_acc: 0.6403\n",
      "Epoch 52/250\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7231 - acc: 0.6809 - val_loss: 0.7363 - val_acc: 0.6462\n",
      "Epoch 53/250\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.7173 - acc: 0.6817 - val_loss: 0.7295 - val_acc: 0.6566\n",
      "Epoch 54/250\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.7081 - acc: 0.6942 - val_loss: 0.7228 - val_acc: 0.6654\n",
      "Epoch 55/250\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6992 - acc: 0.7038 - val_loss: 0.7160 - val_acc: 0.6728\n",
      "Epoch 56/250\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6988 - acc: 0.6950 - val_loss: 0.7092 - val_acc: 0.6832\n",
      "Epoch 57/250\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6871 - acc: 0.7097 - val_loss: 0.7025 - val_acc: 0.6913\n",
      "Epoch 58/250\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.6829 - acc: 0.7127 - val_loss: 0.6957 - val_acc: 0.6979\n",
      "Epoch 59/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6799 - acc: 0.6965 - val_loss: 0.6889 - val_acc: 0.7001\n",
      "Epoch 60/250\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.6705 - acc: 0.7282 - val_loss: 0.6821 - val_acc: 0.7053\n",
      "Epoch 61/250\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.6561 - acc: 0.7578 - val_loss: 0.6754 - val_acc: 0.7186\n",
      "Epoch 62/250\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6531 - acc: 0.7489 - val_loss: 0.6687 - val_acc: 0.7208\n",
      "Epoch 63/250\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6460 - acc: 0.7504 - val_loss: 0.6620 - val_acc: 0.7282\n",
      "Epoch 64/250\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.6433 - acc: 0.7563 - val_loss: 0.6552 - val_acc: 0.7363\n",
      "Epoch 65/250\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.6336 - acc: 0.7858 - val_loss: 0.6486 - val_acc: 0.7430\n",
      "Epoch 66/250\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6274 - acc: 0.7814 - val_loss: 0.6419 - val_acc: 0.7482\n",
      "Epoch 67/250\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6159 - acc: 0.7851 - val_loss: 0.6353 - val_acc: 0.7548\n",
      "Epoch 68/250\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6107 - acc: 0.7962 - val_loss: 0.6287 - val_acc: 0.7629\n",
      "Epoch 69/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6044 - acc: 0.8006 - val_loss: 0.6222 - val_acc: 0.7674\n",
      "Epoch 70/250\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6000 - acc: 0.8021 - val_loss: 0.6157 - val_acc: 0.7725\n",
      "Epoch 71/250\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5966 - acc: 0.7895 - val_loss: 0.6093 - val_acc: 0.7770\n",
      "Epoch 72/250\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.5833 - acc: 0.7999 - val_loss: 0.6029 - val_acc: 0.7770\n",
      "Epoch 73/250\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.5812 - acc: 0.8013 - val_loss: 0.5966 - val_acc: 0.7836\n",
      "Epoch 74/250\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5739 - acc: 0.8154 - val_loss: 0.5904 - val_acc: 0.7888\n",
      "Epoch 75/250\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5662 - acc: 0.8183 - val_loss: 0.5841 - val_acc: 0.7910\n",
      "Epoch 76/250\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5634 - acc: 0.8035 - val_loss: 0.5779 - val_acc: 0.7962\n",
      "Epoch 77/250\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5561 - acc: 0.8257 - val_loss: 0.5717 - val_acc: 0.8028\n",
      "Epoch 78/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5473 - acc: 0.8257 - val_loss: 0.5656 - val_acc: 0.8050\n",
      "Epoch 79/250\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5395 - acc: 0.8434 - val_loss: 0.5595 - val_acc: 0.8080\n",
      "Epoch 80/250\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5312 - acc: 0.8397 - val_loss: 0.5533 - val_acc: 0.8102\n",
      "Epoch 81/250\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5279 - acc: 0.8323 - val_loss: 0.5472 - val_acc: 0.8139\n",
      "Epoch 82/250\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5157 - acc: 0.8626 - val_loss: 0.5410 - val_acc: 0.8154\n",
      "Epoch 83/250\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5199 - acc: 0.8250 - val_loss: 0.5349 - val_acc: 0.8176\n",
      "Epoch 84/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5049 - acc: 0.8589 - val_loss: 0.5288 - val_acc: 0.8191\n",
      "Epoch 85/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5053 - acc: 0.8471 - val_loss: 0.5227 - val_acc: 0.8205\n",
      "Epoch 86/250\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5005 - acc: 0.8582 - val_loss: 0.5167 - val_acc: 0.8235\n",
      "Epoch 87/250\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4911 - acc: 0.8530 - val_loss: 0.5109 - val_acc: 0.8227\n",
      "Epoch 88/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4859 - acc: 0.8552 - val_loss: 0.5051 - val_acc: 0.8242\n",
      "Epoch 89/250\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4798 - acc: 0.8589 - val_loss: 0.4995 - val_acc: 0.8272\n",
      "Epoch 90/250\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4770 - acc: 0.8641 - val_loss: 0.4941 - val_acc: 0.8323\n",
      "Epoch 91/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4637 - acc: 0.8715 - val_loss: 0.4888 - val_acc: 0.8353\n",
      "Epoch 92/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4609 - acc: 0.8715 - val_loss: 0.4836 - val_acc: 0.8383\n",
      "Epoch 93/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4526 - acc: 0.8685 - val_loss: 0.4785 - val_acc: 0.8397\n",
      "Epoch 94/250\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4574 - acc: 0.8722 - val_loss: 0.4735 - val_acc: 0.8434\n",
      "Epoch 95/250\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4463 - acc: 0.8700 - val_loss: 0.4686 - val_acc: 0.8464\n",
      "Epoch 96/250\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4356 - acc: 0.8744 - val_loss: 0.4638 - val_acc: 0.8471\n",
      "Epoch 97/250\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4401 - acc: 0.8796 - val_loss: 0.4591 - val_acc: 0.8493\n",
      "Epoch 98/250\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4286 - acc: 0.8774 - val_loss: 0.4546 - val_acc: 0.8501\n",
      "Epoch 99/250\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4231 - acc: 0.8900 - val_loss: 0.4501 - val_acc: 0.8501\n",
      "Epoch 100/250\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4286 - acc: 0.8759 - val_loss: 0.4457 - val_acc: 0.8523\n",
      "Epoch 101/250\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4194 - acc: 0.8826 - val_loss: 0.4414 - val_acc: 0.8530\n",
      "Epoch 102/250\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4149 - acc: 0.8804 - val_loss: 0.4372 - val_acc: 0.8538\n",
      "Epoch 103/250\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4091 - acc: 0.8877 - val_loss: 0.4330 - val_acc: 0.8560\n",
      "Epoch 104/250\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4049 - acc: 0.8855 - val_loss: 0.4289 - val_acc: 0.8582\n",
      "Epoch 105/250\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4018 - acc: 0.8796 - val_loss: 0.4248 - val_acc: 0.8582\n",
      "Epoch 106/250\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3902 - acc: 0.8877 - val_loss: 0.4209 - val_acc: 0.8604\n",
      "Epoch 107/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3891 - acc: 0.8922 - val_loss: 0.4169 - val_acc: 0.8626\n",
      "Epoch 108/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3816 - acc: 0.8966 - val_loss: 0.4131 - val_acc: 0.8634\n",
      "Epoch 109/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3827 - acc: 0.8907 - val_loss: 0.4094 - val_acc: 0.8634\n",
      "Epoch 110/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3746 - acc: 0.8907 - val_loss: 0.4058 - val_acc: 0.8641\n",
      "Epoch 111/250\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3768 - acc: 0.8959 - val_loss: 0.4023 - val_acc: 0.8634\n",
      "Epoch 112/250\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.3710 - acc: 0.8959 - val_loss: 0.3989 - val_acc: 0.8641\n",
      "Epoch 113/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3651 - acc: 0.8936 - val_loss: 0.3956 - val_acc: 0.8641\n",
      "Epoch 114/250\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3671 - acc: 0.9032 - val_loss: 0.3924 - val_acc: 0.8634\n",
      "Epoch 115/250\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3610 - acc: 0.8929 - val_loss: 0.3893 - val_acc: 0.8626\n",
      "Epoch 116/250\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3554 - acc: 0.8966 - val_loss: 0.3862 - val_acc: 0.8634\n",
      "Epoch 117/250\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3584 - acc: 0.8973 - val_loss: 0.3833 - val_acc: 0.8619\n",
      "Epoch 118/250\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3509 - acc: 0.9010 - val_loss: 0.3804 - val_acc: 0.8619\n",
      "Epoch 119/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3448 - acc: 0.9040 - val_loss: 0.3776 - val_acc: 0.8634\n",
      "Epoch 120/250\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3392 - acc: 0.8944 - val_loss: 0.3749 - val_acc: 0.8648\n",
      "Epoch 121/250\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3433 - acc: 0.8988 - val_loss: 0.3723 - val_acc: 0.8648\n",
      "Epoch 122/250\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3348 - acc: 0.9069 - val_loss: 0.3698 - val_acc: 0.8663\n",
      "Epoch 123/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3329 - acc: 0.9047 - val_loss: 0.3673 - val_acc: 0.8678\n",
      "Epoch 124/250\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.3256 - acc: 0.9084 - val_loss: 0.3649 - val_acc: 0.8700\n",
      "Epoch 125/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3281 - acc: 0.9003 - val_loss: 0.3625 - val_acc: 0.8700\n",
      "Epoch 126/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3258 - acc: 0.9040 - val_loss: 0.3602 - val_acc: 0.8693\n",
      "Epoch 127/250\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3227 - acc: 0.9032 - val_loss: 0.3578 - val_acc: 0.8700\n",
      "Epoch 128/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3173 - acc: 0.9077 - val_loss: 0.3556 - val_acc: 0.8700\n",
      "Epoch 129/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3126 - acc: 0.9136 - val_loss: 0.3535 - val_acc: 0.8700\n",
      "Epoch 130/250\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3121 - acc: 0.9106 - val_loss: 0.3513 - val_acc: 0.8708\n",
      "Epoch 131/250\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3113 - acc: 0.9055 - val_loss: 0.3493 - val_acc: 0.8715\n",
      "Epoch 132/250\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3166 - acc: 0.9077 - val_loss: 0.3473 - val_acc: 0.8722\n",
      "Epoch 133/250\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3089 - acc: 0.9114 - val_loss: 0.3454 - val_acc: 0.8744\n",
      "Epoch 134/250\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3067 - acc: 0.9069 - val_loss: 0.3435 - val_acc: 0.8737\n",
      "Epoch 135/250\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3021 - acc: 0.9114 - val_loss: 0.3418 - val_acc: 0.8752\n",
      "Epoch 136/250\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2999 - acc: 0.9106 - val_loss: 0.3400 - val_acc: 0.8752\n",
      "Epoch 137/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3012 - acc: 0.9077 - val_loss: 0.3384 - val_acc: 0.8759\n",
      "Epoch 138/250\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2999 - acc: 0.9129 - val_loss: 0.3369 - val_acc: 0.8774\n",
      "Epoch 139/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2887 - acc: 0.9232 - val_loss: 0.3354 - val_acc: 0.8767\n",
      "Epoch 140/250\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2921 - acc: 0.9084 - val_loss: 0.3339 - val_acc: 0.8774\n",
      "Epoch 141/250\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.2948 - acc: 0.9165 - val_loss: 0.3324 - val_acc: 0.8781\n",
      "Epoch 142/250\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2911 - acc: 0.9136 - val_loss: 0.3310 - val_acc: 0.8789\n",
      "Epoch 143/250\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2850 - acc: 0.9143 - val_loss: 0.3296 - val_acc: 0.8804\n",
      "Epoch 144/250\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.2876 - acc: 0.9173 - val_loss: 0.3283 - val_acc: 0.8796\n",
      "Epoch 145/250\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.2847 - acc: 0.9151 - val_loss: 0.3270 - val_acc: 0.8789\n",
      "Epoch 146/250\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2826 - acc: 0.9180 - val_loss: 0.3257 - val_acc: 0.8796\n",
      "Epoch 147/250\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2801 - acc: 0.9136 - val_loss: 0.3245 - val_acc: 0.8796\n",
      "Epoch 148/250\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.2826 - acc: 0.9121 - val_loss: 0.3233 - val_acc: 0.8796\n",
      "Epoch 149/250\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2810 - acc: 0.9151 - val_loss: 0.3222 - val_acc: 0.8781\n",
      "Epoch 150/250\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2739 - acc: 0.9188 - val_loss: 0.3211 - val_acc: 0.8781\n",
      "Epoch 151/250\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2784 - acc: 0.9151 - val_loss: 0.3201 - val_acc: 0.8781\n",
      "Epoch 152/250\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2707 - acc: 0.9195 - val_loss: 0.3190 - val_acc: 0.8774\n",
      "Epoch 153/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2709 - acc: 0.9232 - val_loss: 0.3180 - val_acc: 0.8781\n",
      "Epoch 154/250\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.2657 - acc: 0.9247 - val_loss: 0.3169 - val_acc: 0.8796\n",
      "Epoch 155/250\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2662 - acc: 0.9247 - val_loss: 0.3158 - val_acc: 0.8796\n",
      "Epoch 156/250\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2657 - acc: 0.9210 - val_loss: 0.3148 - val_acc: 0.8804\n",
      "Epoch 157/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2679 - acc: 0.9202 - val_loss: 0.3138 - val_acc: 0.8796\n",
      "Epoch 158/250\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.2648 - acc: 0.9188 - val_loss: 0.3129 - val_acc: 0.8796\n",
      "Epoch 159/250\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2615 - acc: 0.9158 - val_loss: 0.3119 - val_acc: 0.8796\n",
      "Epoch 160/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2584 - acc: 0.9158 - val_loss: 0.3110 - val_acc: 0.8789\n",
      "Epoch 161/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2608 - acc: 0.9254 - val_loss: 0.3101 - val_acc: 0.8789\n",
      "Epoch 162/250\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.2544 - acc: 0.9239 - val_loss: 0.3092 - val_acc: 0.8781\n",
      "Epoch 163/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2544 - acc: 0.9239 - val_loss: 0.3083 - val_acc: 0.8789\n",
      "Epoch 164/250\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2633 - acc: 0.9106 - val_loss: 0.3075 - val_acc: 0.8781\n",
      "Epoch 165/250\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2502 - acc: 0.9188 - val_loss: 0.3067 - val_acc: 0.8796\n",
      "Epoch 166/250\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2545 - acc: 0.9254 - val_loss: 0.3059 - val_acc: 0.8804\n",
      "Epoch 167/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2518 - acc: 0.9188 - val_loss: 0.3051 - val_acc: 0.8796\n",
      "Epoch 168/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2490 - acc: 0.9173 - val_loss: 0.3044 - val_acc: 0.8796\n",
      "Epoch 169/250\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2503 - acc: 0.9217 - val_loss: 0.3036 - val_acc: 0.8796\n",
      "Epoch 170/250\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2508 - acc: 0.9239 - val_loss: 0.3028 - val_acc: 0.8796\n",
      "Epoch 171/250\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2442 - acc: 0.9269 - val_loss: 0.3022 - val_acc: 0.8789\n",
      "Epoch 172/250\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2513 - acc: 0.9210 - val_loss: 0.3015 - val_acc: 0.8774\n",
      "Epoch 173/250\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2472 - acc: 0.9239 - val_loss: 0.3009 - val_acc: 0.8774\n",
      "Epoch 174/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2503 - acc: 0.9143 - val_loss: 0.3003 - val_acc: 0.8774\n",
      "Epoch 175/250\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.2411 - acc: 0.9261 - val_loss: 0.2997 - val_acc: 0.8774\n",
      "Epoch 176/250\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.2400 - acc: 0.9284 - val_loss: 0.2991 - val_acc: 0.8774\n",
      "Epoch 177/250\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2411 - acc: 0.9232 - val_loss: 0.2985 - val_acc: 0.8774\n",
      "Epoch 178/250\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2365 - acc: 0.9313 - val_loss: 0.2980 - val_acc: 0.8781\n",
      "Epoch 179/250\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.2415 - acc: 0.9210 - val_loss: 0.2975 - val_acc: 0.8789\n",
      "Epoch 180/250\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2391 - acc: 0.9254 - val_loss: 0.2970 - val_acc: 0.8789\n",
      "Epoch 181/250\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2424 - acc: 0.9269 - val_loss: 0.2965 - val_acc: 0.8789\n",
      "Epoch 182/250\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2388 - acc: 0.9335 - val_loss: 0.2960 - val_acc: 0.8781\n",
      "Epoch 183/250\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2365 - acc: 0.9232 - val_loss: 0.2956 - val_acc: 0.8781\n",
      "Epoch 184/250\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.2332 - acc: 0.9232 - val_loss: 0.2951 - val_acc: 0.8781\n",
      "Epoch 185/250\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2358 - acc: 0.9328 - val_loss: 0.2947 - val_acc: 0.8781\n",
      "Epoch 186/250\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2291 - acc: 0.9284 - val_loss: 0.2942 - val_acc: 0.8774\n",
      "Epoch 187/250\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2329 - acc: 0.9247 - val_loss: 0.2936 - val_acc: 0.8774\n",
      "Epoch 188/250\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2276 - acc: 0.9357 - val_loss: 0.2930 - val_acc: 0.8767\n",
      "Epoch 189/250\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2283 - acc: 0.9254 - val_loss: 0.2924 - val_acc: 0.8767\n",
      "Epoch 190/250\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2290 - acc: 0.9225 - val_loss: 0.2919 - val_acc: 0.8767\n",
      "Epoch 191/250\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2268 - acc: 0.9276 - val_loss: 0.2914 - val_acc: 0.8767\n",
      "Epoch 192/250\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2264 - acc: 0.9269 - val_loss: 0.2910 - val_acc: 0.8767\n",
      "Epoch 193/250\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2283 - acc: 0.9254 - val_loss: 0.2907 - val_acc: 0.8759\n",
      "Epoch 194/250\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.2238 - acc: 0.9284 - val_loss: 0.2904 - val_acc: 0.8781\n",
      "Epoch 195/250\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2254 - acc: 0.9217 - val_loss: 0.2902 - val_acc: 0.8774\n",
      "Epoch 196/250\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2249 - acc: 0.9269 - val_loss: 0.2901 - val_acc: 0.8796\n",
      "Epoch 197/250\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2241 - acc: 0.9298 - val_loss: 0.2899 - val_acc: 0.8804\n",
      "Epoch 198/250\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2252 - acc: 0.9276 - val_loss: 0.2897 - val_acc: 0.8796\n",
      "Epoch 199/250\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2221 - acc: 0.9357 - val_loss: 0.2894 - val_acc: 0.8796\n",
      "Epoch 200/250\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2182 - acc: 0.9313 - val_loss: 0.2891 - val_acc: 0.8789\n",
      "Epoch 201/250\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2199 - acc: 0.9380 - val_loss: 0.2888 - val_acc: 0.8811\n",
      "Epoch 202/250\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2172 - acc: 0.9276 - val_loss: 0.2884 - val_acc: 0.8804\n",
      "Epoch 203/250\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2186 - acc: 0.9276 - val_loss: 0.2878 - val_acc: 0.8804\n",
      "Epoch 204/250\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2197 - acc: 0.9343 - val_loss: 0.2874 - val_acc: 0.8774\n",
      "Epoch 205/250\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2202 - acc: 0.9269 - val_loss: 0.2869 - val_acc: 0.8781\n",
      "Epoch 206/250\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2188 - acc: 0.9284 - val_loss: 0.2864 - val_acc: 0.8781\n",
      "Epoch 207/250\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2188 - acc: 0.9298 - val_loss: 0.2860 - val_acc: 0.8781\n",
      "Epoch 208/250\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2147 - acc: 0.9298 - val_loss: 0.2856 - val_acc: 0.8789\n",
      "Epoch 209/250\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2166 - acc: 0.9254 - val_loss: 0.2852 - val_acc: 0.8789\n",
      "Epoch 210/250\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2147 - acc: 0.9306 - val_loss: 0.2848 - val_acc: 0.8789\n",
      "Epoch 211/250\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2117 - acc: 0.9335 - val_loss: 0.2845 - val_acc: 0.8796\n",
      "Epoch 212/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2075 - acc: 0.9357 - val_loss: 0.2841 - val_acc: 0.8804\n",
      "Epoch 213/250\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.2103 - acc: 0.9284 - val_loss: 0.2838 - val_acc: 0.8804\n",
      "Epoch 214/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2110 - acc: 0.9269 - val_loss: 0.2834 - val_acc: 0.8804\n",
      "Epoch 215/250\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2096 - acc: 0.9284 - val_loss: 0.2832 - val_acc: 0.8796\n",
      "Epoch 216/250\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2113 - acc: 0.9261 - val_loss: 0.2829 - val_acc: 0.8796\n",
      "Epoch 217/250\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2124 - acc: 0.9328 - val_loss: 0.2828 - val_acc: 0.8796\n",
      "Epoch 218/250\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2104 - acc: 0.9321 - val_loss: 0.2827 - val_acc: 0.8796\n",
      "Epoch 219/250\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2081 - acc: 0.9328 - val_loss: 0.2826 - val_acc: 0.8781\n",
      "Epoch 220/250\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2053 - acc: 0.9276 - val_loss: 0.2824 - val_acc: 0.8781\n",
      "Epoch 221/250\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2066 - acc: 0.9365 - val_loss: 0.2822 - val_acc: 0.8781\n",
      "Epoch 222/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2104 - acc: 0.9357 - val_loss: 0.2820 - val_acc: 0.8789\n",
      "Epoch 223/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2076 - acc: 0.9343 - val_loss: 0.2817 - val_acc: 0.8796\n",
      "Epoch 224/250\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2105 - acc: 0.9335 - val_loss: 0.2815 - val_acc: 0.8804\n",
      "Epoch 225/250\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2047 - acc: 0.9313 - val_loss: 0.2812 - val_acc: 0.8804\n",
      "Epoch 226/250\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2044 - acc: 0.9291 - val_loss: 0.2810 - val_acc: 0.8804\n",
      "Epoch 227/250\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2057 - acc: 0.9321 - val_loss: 0.2808 - val_acc: 0.8796\n",
      "Epoch 228/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2029 - acc: 0.9291 - val_loss: 0.2806 - val_acc: 0.8796\n",
      "Epoch 229/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2046 - acc: 0.9291 - val_loss: 0.2804 - val_acc: 0.8796\n",
      "Epoch 230/250\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.2078 - acc: 0.9328 - val_loss: 0.2802 - val_acc: 0.8796\n",
      "Epoch 231/250\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.2022 - acc: 0.9328 - val_loss: 0.2800 - val_acc: 0.8789\n",
      "Epoch 232/250\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2022 - acc: 0.9269 - val_loss: 0.2798 - val_acc: 0.8796\n",
      "Epoch 233/250\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2040 - acc: 0.9350 - val_loss: 0.2796 - val_acc: 0.8804\n",
      "Epoch 234/250\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1963 - acc: 0.9380 - val_loss: 0.2794 - val_acc: 0.8818\n",
      "Epoch 235/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2036 - acc: 0.9328 - val_loss: 0.2791 - val_acc: 0.8811\n",
      "Epoch 236/250\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1968 - acc: 0.9350 - val_loss: 0.2789 - val_acc: 0.8789\n",
      "Epoch 237/250\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.1992 - acc: 0.9365 - val_loss: 0.2787 - val_acc: 0.8789\n",
      "Epoch 238/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.1952 - acc: 0.9343 - val_loss: 0.2785 - val_acc: 0.8781\n",
      "Epoch 239/250\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1983 - acc: 0.9365 - val_loss: 0.2784 - val_acc: 0.8789\n",
      "Epoch 240/250\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1957 - acc: 0.9350 - val_loss: 0.2782 - val_acc: 0.8789\n",
      "Epoch 241/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2049 - acc: 0.9335 - val_loss: 0.2780 - val_acc: 0.8796\n",
      "Epoch 242/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.1963 - acc: 0.9335 - val_loss: 0.2778 - val_acc: 0.8796\n",
      "Epoch 243/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.1944 - acc: 0.9365 - val_loss: 0.2776 - val_acc: 0.8789\n",
      "Epoch 244/250\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.2015 - acc: 0.9387 - val_loss: 0.2776 - val_acc: 0.8789\n",
      "Epoch 245/250\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1936 - acc: 0.9409 - val_loss: 0.2775 - val_acc: 0.8789\n",
      "Epoch 246/250\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1928 - acc: 0.9380 - val_loss: 0.2773 - val_acc: 0.8796\n",
      "Epoch 247/250\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1916 - acc: 0.9357 - val_loss: 0.2771 - val_acc: 0.8796\n",
      "Epoch 248/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1939 - acc: 0.9343 - val_loss: 0.2770 - val_acc: 0.8789\n",
      "Epoch 249/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1968 - acc: 0.9298 - val_loss: 0.2769 - val_acc: 0.8796\n",
      "Epoch 250/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1928 - acc: 0.9372 - val_loss: 0.2768 - val_acc: 0.8804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc680144350>"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.train(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model {graph}]\n",
      "                             [--layer_number LAYER_NUMBER] [--dataset {cora}]\n",
      "                             [--cora_path CORA_PATH]\n",
      "                             [--output_path OUTPUT_PATH] [--channels CHANNELS]\n",
      "                             [--dropout DROPOUT] [--l2_reg L2_REG]\n",
      "                             [--epochs EPOCHS] [--lr LR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/x5zhan/.local/share/jupyter/runtime/kernel-7dce758e-8ff2-4c92-8077-b00d08365723.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import argparse\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='Running model')\n",
    "    parser.add_argument('--model', type=str, default='graph', choices=['graph'],\n",
    "                        help='model to use for training (default: nlayerGNN)')\n",
    "    parser.add_argument('--layer_number', type=int, default=1,\n",
    "                        help='input layer number for nlayerGNN')\n",
    "    \n",
    "    parser.add_argument('--dataset', type=str, default='cora', choices=['cora'],\n",
    "                        help='data set type (default cora and only support cora now)')\n",
    "    parser.add_argument('--cora_path', type=str, default='/data',\n",
    "                        help='path for the cora dataset')\n",
    "    parser.add_argument('--output_path', type=str, default='/config/model-output.json',\n",
    "                        help='path for the output json file')\n",
    "    \n",
    "    parser.add_argument('--channels', type=int, default=16,\n",
    "                        help='channels output of each layer (GNN) (default: 16)')\n",
    "    parser.add_argument('--dropout', type=int, default=0.1,\n",
    "                        help='dropout ratio in dropout layer (default: 0.1)')\n",
    "    parser.add_argument('--l2_reg', type=int, default=5e-4,\n",
    "                        help='l2 regularzaytion (default: 5e-4)')\n",
    "    \n",
    "    parser.add_argument('--epochs', type=int, default=50,\n",
    "                        help='number of epochs to train (default: 50)')\n",
    "    parser.add_argument('--lr', type=float, default=1e-3,\n",
    "                        help='learning rate (default: 1e-3)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    cora = cora_loader(args.cora_path + '/cora.content', args.cora_path + '/cora.cites')\n",
    "    model = gnn_nlayer(channels = args.channels, dropout = args.dropout, l2_reg = args.l2_reg)\n",
    "    X, y, A = cora.get_train()\n",
    "    model.fit(X, y, A)\n",
    "    model.mcompile(optimizer='adam', loss='cr', learning_rate=args.lr)\n",
    "    hist = model.train(args.epochs)\n",
    "    with open(args.output_path, 'w') as f:\n",
    "        json.dump(hist.history, f)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    # Examples:\n",
    "    # python run.py --model graph --dataset cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
