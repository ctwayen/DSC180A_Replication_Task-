{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from layers import LPA_GCN_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LPA_GCN_model(nn.Module):\n",
    "    def __init__(self, A, len_walk, F, class_number, hid):\n",
    "        super(LPA_GCN_model, self).__init__()\n",
    "        self.lg1 = LPA_GCN_layer(F = F, O = hid, A=A, len_walk=len_walk)\n",
    "        self.lg2 = LPA_GCN_layer(F = hid, O = class_number, A=A, len_walk=len_walk)\n",
    "\n",
    "    def forward(self, X, A, y):\n",
    "        X, y_hat = self.lg1(X, A, y)\n",
    "        X = F.relu(X)\n",
    "        X, y_hat = self.lg2(X, A, y_hat)\n",
    "        return F.log_softmax(X, dim=1), F.log_softmax(y_hat,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LPA_GCN():\n",
    "    def __init__(self, A, X, y, lamb, device='cuda', len_walk=2, F=1433, class_number=7, hid=200, val = 0.3):\n",
    "        if device == 'cuda':\n",
    "            self.device= torch.device('cuda')\n",
    "        else:\n",
    "            assert('only support cuda')\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        one = preprocessing.OneHotEncoder(sparse=False)\n",
    "        y_ = np.reshape(y, (-1, 1))\n",
    "        one.fit(y_)\n",
    "        labels = np.array(one.transform(y_))\n",
    "        labels = torch.from_numpy(labels).type(torch.float)\n",
    "        le.fit(y)\n",
    "        y = le.transform(y)\n",
    "        \n",
    "        X = torch.tensor(X)\n",
    "        X = X.type(torch.float)\n",
    "        y = torch.tensor(y)\n",
    "        y = y.type(torch.long)\n",
    "        y = y.to(self.device)\n",
    "        A = torch.from_numpy(A).float()\n",
    "        \n",
    "        self.X = X.to(self.device)\n",
    "        self.A = A.to(self.device)\n",
    "        self.y = y.to(self.device)\n",
    "        self.Lambda = lamb\n",
    "        self.labels = labels.to(self.device)\n",
    "        \n",
    "        train_idx = np.random.choice(self.X.shape[0], round(self.X.shape[0]*(1-val)), replace=False)\n",
    "        val_idx = np.array([x for x in range(X.shape[0]) if x not in train_idx])\n",
    "        print(\"Train length :{a}, Validation length :{b}\".format(a=len(train_idx), b=len(val_idx)))\n",
    "        \n",
    "        self.idx_train = torch.LongTensor(train_idx)\n",
    "        self.idx_val = torch.LongTensor(val_idx)\n",
    "        \n",
    "        self.lpa_gcn = LPA_GCN_model(A = self.A, len_walk=len_walk, F=F, class_number = class_number, hid = hid)\n",
    "        self.lpa_gcn.to(self.device)\n",
    "    \n",
    "    def train(self, optimizer, epoch):\n",
    "        self.lpa_gcn.train()\n",
    "        optimizer.zero_grad()\n",
    "        output, y_hat = self.lpa_gcn(self.X, self.A, self.labels)\n",
    "        loss_gcn = F.cross_entropy(output[self.idx_train], self.y[self.idx_train])\n",
    "        loss_lpa = F.nll_loss(y_hat, self.y)\n",
    "        loss_train = loss_gcn + self.Lambda * loss_lpa\n",
    "        loss_train.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        print('Epoch: {x}'.format(x=epoch))\n",
    "        print('training loss {:.4f}'.format(loss_train.item()))\n",
    "            \n",
    "    def test(self):\n",
    "        self.lpa_gcn.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            output, y_hat = self.lpa_gcn(self.X, self.A, self.labels)\n",
    "            #print(self.idx_val)\n",
    "            test_loss = F.cross_entropy(output[self.idx_val], self.y[self.idx_val], reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)[self.idx_val]\n",
    "            correct += pred.eq(self.y[self.idx_val].view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(self.idx_val)\n",
    "        print('Average loss: {:.4f}, Accuracy: {:.4f}%'.format(test_loss, 100. * correct / len(self.idx_val)))\n",
    "        return 100. * correct / len(self.idx_val)\n",
    "    \n",
    "    def train_model(self, epochs=50, lr=1e-3):\n",
    "        lpa_gcn_test_acc = []\n",
    "        optimizer = optim.Adam(self.lpa_gcn.parameters(), lr=lr)#, weight_decay=1e-1)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train(optimizer, epoch)\n",
    "            accs = self.test()\n",
    "            lpa_gcn_test_acc.append(accs)\n",
    "        accs = {'acc': lpa_gcn_test_acc}\n",
    "        return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coraloader import cora_loader\n",
    "cora = cora_loader('data' + '/cora.content', 'data' + '/cora.cites', None)\n",
    "X, y, A = cora.get_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length :1896, Validation length :812\n"
     ]
    }
   ],
   "source": [
    "test = LPA_GCN(A, X, y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "training loss 3.5086\n",
      "Average loss: 1.9807, Accuracy: 17.9803%\n",
      "Epoch: 1\n",
      "training loss 3.3653\n",
      "Average loss: 1.8593, Accuracy: 30.5419%\n",
      "Epoch: 2\n",
      "training loss 3.2347\n",
      "Average loss: 1.7499, Accuracy: 50.9852%\n",
      "Epoch: 3\n",
      "training loss 3.1169\n",
      "Average loss: 1.6522, Accuracy: 55.9113%\n",
      "Epoch: 4\n",
      "training loss 3.0114\n",
      "Average loss: 1.5645, Accuracy: 56.0345%\n",
      "Epoch: 5\n",
      "training loss 2.9165\n",
      "Average loss: 1.4850, Accuracy: 54.8030%\n",
      "Epoch: 6\n",
      "training loss 2.8302\n",
      "Average loss: 1.4112, Accuracy: 55.0493%\n",
      "Epoch: 7\n",
      "training loss 2.7501\n",
      "Average loss: 1.3408, Accuracy: 56.2808%\n",
      "Epoch: 8\n",
      "training loss 2.6739\n",
      "Average loss: 1.2718, Accuracy: 58.7438%\n",
      "Epoch: 9\n",
      "training loss 2.5998\n",
      "Average loss: 1.2034, Accuracy: 62.4384%\n",
      "Epoch: 10\n",
      "training loss 2.5266\n",
      "Average loss: 1.1355, Accuracy: 66.1330%\n",
      "Epoch: 11\n",
      "training loss 2.4543\n",
      "Average loss: 1.0687, Accuracy: 70.8128%\n",
      "Epoch: 12\n",
      "training loss 2.3835\n",
      "Average loss: 1.0044, Accuracy: 73.3990%\n",
      "Epoch: 13\n",
      "training loss 2.3157\n",
      "Average loss: 0.9439, Accuracy: 76.2315%\n",
      "Epoch: 14\n",
      "training loss 2.2520\n",
      "Average loss: 0.8880, Accuracy: 78.5714%\n",
      "Epoch: 15\n",
      "training loss 2.1932\n",
      "Average loss: 0.8367, Accuracy: 79.9261%\n",
      "Epoch: 16\n",
      "training loss 2.1392\n",
      "Average loss: 0.7896, Accuracy: 80.7882%\n",
      "Epoch: 17\n",
      "training loss 2.0896\n",
      "Average loss: 0.7464, Accuracy: 82.3892%\n",
      "Epoch: 18\n",
      "training loss 2.0440\n",
      "Average loss: 0.7068, Accuracy: 83.8670%\n",
      "Epoch: 19\n",
      "training loss 2.0022\n",
      "Average loss: 0.6706, Accuracy: 84.3596%\n",
      "Epoch: 20\n",
      "training loss 1.9639\n",
      "Average loss: 0.6379, Accuracy: 85.3448%\n",
      "Epoch: 21\n",
      "training loss 1.9290\n",
      "Average loss: 0.6086, Accuracy: 85.5911%\n",
      "Epoch: 22\n",
      "training loss 1.8974\n",
      "Average loss: 0.5828, Accuracy: 86.5764%\n",
      "Epoch: 23\n",
      "training loss 1.8691\n",
      "Average loss: 0.5603, Accuracy: 86.6995%\n",
      "Epoch: 24\n",
      "training loss 1.8439\n",
      "Average loss: 0.5410, Accuracy: 86.9458%\n",
      "Epoch: 25\n",
      "training loss 1.8216\n",
      "Average loss: 0.5243, Accuracy: 87.1921%\n",
      "Epoch: 26\n",
      "training loss 1.8016\n",
      "Average loss: 0.5098, Accuracy: 87.0690%\n",
      "Epoch: 27\n",
      "training loss 1.7836\n",
      "Average loss: 0.4973, Accuracy: 86.9458%\n",
      "Epoch: 28\n",
      "training loss 1.7672\n",
      "Average loss: 0.4862, Accuracy: 86.9458%\n",
      "Epoch: 29\n",
      "training loss 1.7521\n",
      "Average loss: 0.4764, Accuracy: 86.8227%\n",
      "Epoch: 30\n",
      "training loss 1.7380\n",
      "Average loss: 0.4677, Accuracy: 86.6995%\n",
      "Epoch: 31\n",
      "training loss 1.7248\n",
      "Average loss: 0.4600, Accuracy: 86.6995%\n",
      "Epoch: 32\n",
      "training loss 1.7126\n",
      "Average loss: 0.4533, Accuracy: 86.6995%\n",
      "Epoch: 33\n",
      "training loss 1.7012\n",
      "Average loss: 0.4474, Accuracy: 86.8227%\n",
      "Epoch: 34\n",
      "training loss 1.6906\n",
      "Average loss: 0.4422, Accuracy: 86.8227%\n",
      "Epoch: 35\n",
      "training loss 1.6808\n",
      "Average loss: 0.4377, Accuracy: 86.9458%\n",
      "Epoch: 36\n",
      "training loss 1.6717\n",
      "Average loss: 0.4339, Accuracy: 86.8227%\n",
      "Epoch: 37\n",
      "training loss 1.6631\n",
      "Average loss: 0.4305, Accuracy: 86.9458%\n",
      "Epoch: 38\n",
      "training loss 1.6551\n",
      "Average loss: 0.4276, Accuracy: 86.9458%\n",
      "Epoch: 39\n",
      "training loss 1.6475\n",
      "Average loss: 0.4252, Accuracy: 86.9458%\n",
      "Epoch: 40\n",
      "training loss 1.6403\n",
      "Average loss: 0.4231, Accuracy: 87.1921%\n",
      "Epoch: 41\n",
      "training loss 1.6335\n",
      "Average loss: 0.4213, Accuracy: 87.0690%\n",
      "Epoch: 42\n",
      "training loss 1.6271\n",
      "Average loss: 0.4199, Accuracy: 87.5616%\n",
      "Epoch: 43\n",
      "training loss 1.6210\n",
      "Average loss: 0.4187, Accuracy: 87.4384%\n",
      "Epoch: 44\n",
      "training loss 1.6152\n",
      "Average loss: 0.4178, Accuracy: 87.5616%\n",
      "Epoch: 45\n",
      "training loss 1.6096\n",
      "Average loss: 0.4171, Accuracy: 87.5616%\n",
      "Epoch: 46\n",
      "training loss 1.6042\n",
      "Average loss: 0.4165, Accuracy: 87.0690%\n",
      "Epoch: 47\n",
      "training loss 1.5991\n",
      "Average loss: 0.4160, Accuracy: 87.0690%\n",
      "Epoch: 48\n",
      "training loss 1.5942\n",
      "Average loss: 0.4157, Accuracy: 87.0690%\n",
      "Epoch: 49\n",
      "training loss 1.5895\n",
      "Average loss: 0.4154, Accuracy: 86.9458%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': [17.980295566502463,\n",
       "  30.541871921182267,\n",
       "  50.98522167487685,\n",
       "  55.91133004926108,\n",
       "  56.03448275862069,\n",
       "  54.80295566502463,\n",
       "  55.04926108374384,\n",
       "  56.2807881773399,\n",
       "  58.74384236453202,\n",
       "  62.4384236453202,\n",
       "  66.13300492610837,\n",
       "  70.8128078817734,\n",
       "  73.39901477832512,\n",
       "  76.23152709359606,\n",
       "  78.57142857142857,\n",
       "  79.92610837438424,\n",
       "  80.78817733990148,\n",
       "  82.38916256157636,\n",
       "  83.86699507389163,\n",
       "  84.35960591133005,\n",
       "  85.34482758620689,\n",
       "  85.5911330049261,\n",
       "  86.57635467980296,\n",
       "  86.69950738916256,\n",
       "  86.94581280788178,\n",
       "  87.19211822660098,\n",
       "  87.06896551724138,\n",
       "  86.94581280788178,\n",
       "  86.94581280788178,\n",
       "  86.82266009852216,\n",
       "  86.69950738916256,\n",
       "  86.69950738916256,\n",
       "  86.69950738916256,\n",
       "  86.82266009852216,\n",
       "  86.82266009852216,\n",
       "  86.94581280788178,\n",
       "  86.82266009852216,\n",
       "  86.94581280788178,\n",
       "  86.94581280788178,\n",
       "  86.94581280788178,\n",
       "  87.19211822660098,\n",
       "  87.06896551724138,\n",
       "  87.5615763546798,\n",
       "  87.4384236453202,\n",
       "  87.5615763546798,\n",
       "  87.5615763546798,\n",
       "  87.06896551724138,\n",
       "  87.06896551724138,\n",
       "  87.06896551724138,\n",
       "  86.94581280788178]}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
