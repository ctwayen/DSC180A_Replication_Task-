{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_N_layer(nn.Module):\n",
    "    def __init__(self, A, N=0, F = 1433, class_number=7, hidden_neurons=200):\n",
    "        super(GCN_N_layer, self).__init__()\n",
    "        self.A = A\n",
    "        self.class_number = class_number\n",
    "        self.N = N\n",
    "        self.hidden = nn.Linear(hidden_neurons, hidden_neurons, bias=True)\n",
    "        self.fc1 = nn.Linear(F, hidden_neurons, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(200, self.class_number, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # training on full x, not batch\n",
    "        x = x.float()\n",
    "        # average all neighboors\n",
    "        #print(x.shape)\n",
    "        #A = self.A.float()\n",
    "        #print(A.shape)\n",
    "        #print(self.X.shape)\n",
    "        #print(A.dtype, self.X.dtype, x.dtype)\n",
    "        x = torch.matmul(self.A, x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.hidden(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class n_hidden_GCN():\n",
    "    def __init__(self, A, X, y, device=\"cuda\", N=0, F=1433, class_number = 7, hidden_neurons=200, self_weight=10, val_size=0.3):\n",
    "        self.device = torch.device(device)\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(y)\n",
    "        y = le.transform(y)\n",
    "        \n",
    "        X = torch.tensor(X)\n",
    "        X = X.type(torch.float)\n",
    "        y = torch.tensor(y)\n",
    "        y = y.type(torch.long)\n",
    "        y = y.to(self.device)\n",
    "        \n",
    "        if A[0][0] == 0:\n",
    "            A = A * 1./self_weight + np.identity(A.shape[0])\n",
    "        A = torch.from_numpy(A).float()\n",
    "        \n",
    "        self.X = X.to(self.device)\n",
    "        self.A = A.to(self.device)\n",
    "        self.y = y.to(self.device)\n",
    "    \n",
    "        self.model = GCN_N_layer(A=self.A, N=N, F=F, class_number=class_number, hidden_neurons=hidden_neurons)\n",
    "        \n",
    "        train_idx = np.random.choice(self.X.shape[0], round(self.X.shape[0]*(1-val_size)), replace=False)\n",
    "        val_idx = np.array([x for x in range(X.shape[0]) if x not in train_idx])\n",
    "        print(\"Train length :{a}, Validation length :{b}\".format(a=len(train_idx), b=len(val_idx)))\n",
    "        \n",
    "        self.idx_train = torch.LongTensor(train_idx)\n",
    "        self.idx_val = torch.LongTensor(val_idx)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def train(self, optimizer, epoch):\n",
    "        self.model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = self.model(self.X)\n",
    "        loss = F.cross_entropy(output[self.idx_train], self.y[self.idx_train])\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        print('Epoch: {x}'.format(x=epoch))\n",
    "        print('training loss {:.4f}'.format(loss.item()))\n",
    "            \n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            output = self.model(self.X)\n",
    "            loss = F.cross_entropy(output[self.idx_val], self.y[self.idx_val], reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)[self.idx_val]\n",
    "            correct += pred.eq(self.y[self.idx_val].view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(self.idx_val)\n",
    "        print('Validtion: Average loss: {:.4f}, Accuracy: {:.4f}%'.format(test_loss, 100. * correct / len(self.idx_val)))\n",
    "        return 100. * correct / len(self.idx_val)\n",
    "    \n",
    "    def train_epoch(self, epochs=100, lr=1e-3):\n",
    "        acc = []\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)#, weight_decay=1e-1)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train(optimizer, epoch)\n",
    "            accs = self.test()\n",
    "            acc.append(accs)\n",
    "        accs = {'acc': acc}\n",
    "        return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coraloader import cora_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = cora_loader('data' + '/cora.content', 'data' + '/cora.cites', None)\n",
    "X, y, A = cora.get_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length :1896, Validation length :812\n"
     ]
    }
   ],
   "source": [
    "model = n_hidden_GCN(A, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "training loss 1.9447\n",
      "Validtion: Average loss: 0.0000, Accuracy: 37.5616%\n",
      "Epoch: 1\n",
      "training loss 1.9193\n",
      "Validtion: Average loss: 0.0000, Accuracy: 39.7783%\n",
      "Epoch: 2\n",
      "training loss 1.8938\n",
      "Validtion: Average loss: 0.0000, Accuracy: 38.6700%\n",
      "Epoch: 3\n",
      "training loss 1.8668\n",
      "Validtion: Average loss: 0.0000, Accuracy: 38.7931%\n",
      "Epoch: 4\n",
      "training loss 1.8377\n",
      "Validtion: Average loss: 0.0000, Accuracy: 38.5468%\n",
      "Epoch: 5\n",
      "training loss 1.8063\n",
      "Validtion: Average loss: 0.0000, Accuracy: 40.1478%\n",
      "Epoch: 6\n",
      "training loss 1.7726\n",
      "Validtion: Average loss: 0.0000, Accuracy: 41.3793%\n",
      "Epoch: 7\n",
      "training loss 1.7368\n",
      "Validtion: Average loss: 0.0000, Accuracy: 42.7340%\n",
      "Epoch: 8\n",
      "training loss 1.6990\n",
      "Validtion: Average loss: 0.0000, Accuracy: 44.8276%\n",
      "Epoch: 9\n",
      "training loss 1.6595\n",
      "Validtion: Average loss: 0.0000, Accuracy: 47.5369%\n",
      "Epoch: 10\n",
      "training loss 1.6184\n",
      "Validtion: Average loss: 0.0000, Accuracy: 50.2463%\n",
      "Epoch: 11\n",
      "training loss 1.5760\n",
      "Validtion: Average loss: 0.0000, Accuracy: 53.2020%\n",
      "Epoch: 12\n",
      "training loss 1.5327\n",
      "Validtion: Average loss: 0.0000, Accuracy: 56.8966%\n",
      "Epoch: 13\n",
      "training loss 1.4884\n",
      "Validtion: Average loss: 0.0000, Accuracy: 58.8670%\n",
      "Epoch: 14\n",
      "training loss 1.4434\n",
      "Validtion: Average loss: 0.0000, Accuracy: 60.9606%\n",
      "Epoch: 15\n",
      "training loss 1.3976\n",
      "Validtion: Average loss: 0.0000, Accuracy: 63.0542%\n",
      "Epoch: 16\n",
      "training loss 1.3513\n",
      "Validtion: Average loss: 0.0000, Accuracy: 64.2857%\n",
      "Epoch: 17\n",
      "training loss 1.3046\n",
      "Validtion: Average loss: 0.0000, Accuracy: 67.3645%\n",
      "Epoch: 18\n",
      "training loss 1.2576\n",
      "Validtion: Average loss: 0.0000, Accuracy: 69.4581%\n",
      "Epoch: 19\n",
      "training loss 1.2107\n",
      "Validtion: Average loss: 0.0000, Accuracy: 70.9360%\n",
      "Epoch: 20\n",
      "training loss 1.1640\n",
      "Validtion: Average loss: 0.0000, Accuracy: 72.4138%\n",
      "Epoch: 21\n",
      "training loss 1.1178\n",
      "Validtion: Average loss: 0.0000, Accuracy: 73.3990%\n",
      "Epoch: 22\n",
      "training loss 1.0721\n",
      "Validtion: Average loss: 0.0000, Accuracy: 74.2611%\n",
      "Epoch: 23\n",
      "training loss 1.0271\n",
      "Validtion: Average loss: 0.0000, Accuracy: 74.8768%\n",
      "Epoch: 24\n",
      "training loss 0.9830\n",
      "Validtion: Average loss: 0.0000, Accuracy: 76.1084%\n",
      "Epoch: 25\n",
      "training loss 0.9399\n",
      "Validtion: Average loss: 0.0000, Accuracy: 76.9704%\n",
      "Epoch: 26\n",
      "training loss 0.8980\n",
      "Validtion: Average loss: 0.0000, Accuracy: 77.8325%\n",
      "Epoch: 27\n",
      "training loss 0.8574\n",
      "Validtion: Average loss: 0.0000, Accuracy: 78.3251%\n",
      "Epoch: 28\n",
      "training loss 0.8180\n",
      "Validtion: Average loss: 0.0000, Accuracy: 78.9409%\n",
      "Epoch: 29\n",
      "training loss 0.7801\n",
      "Validtion: Average loss: 0.0000, Accuracy: 79.4335%\n",
      "Epoch: 30\n",
      "training loss 0.7436\n",
      "Validtion: Average loss: 0.0000, Accuracy: 79.9261%\n",
      "Epoch: 31\n",
      "training loss 0.7085\n",
      "Validtion: Average loss: 0.0000, Accuracy: 80.9113%\n",
      "Epoch: 32\n",
      "training loss 0.6750\n",
      "Validtion: Average loss: 0.0000, Accuracy: 81.5271%\n",
      "Epoch: 33\n",
      "training loss 0.6429\n",
      "Validtion: Average loss: 0.0000, Accuracy: 81.7734%\n",
      "Epoch: 34\n",
      "training loss 0.6123\n",
      "Validtion: Average loss: 0.0000, Accuracy: 81.5271%\n",
      "Epoch: 35\n",
      "training loss 0.5832\n",
      "Validtion: Average loss: 0.0000, Accuracy: 82.2660%\n",
      "Epoch: 36\n",
      "training loss 0.5554\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.0049%\n",
      "Epoch: 37\n",
      "training loss 0.5291\n",
      "Validtion: Average loss: 0.0000, Accuracy: 82.7586%\n",
      "Epoch: 38\n",
      "training loss 0.5041\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3744%\n",
      "Epoch: 39\n",
      "training loss 0.4805\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.1281%\n",
      "Epoch: 40\n",
      "training loss 0.4581\n",
      "Validtion: Average loss: 0.0000, Accuracy: 82.8818%\n",
      "Epoch: 41\n",
      "training loss 0.4368\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.0049%\n",
      "Epoch: 42\n",
      "training loss 0.4168\n",
      "Validtion: Average loss: 0.0000, Accuracy: 82.8818%\n",
      "Epoch: 43\n",
      "training loss 0.3978\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.2512%\n",
      "Epoch: 44\n",
      "training loss 0.3798\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.0049%\n",
      "Epoch: 45\n",
      "training loss 0.3628\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.0049%\n",
      "Epoch: 46\n",
      "training loss 0.3468\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.0049%\n",
      "Epoch: 47\n",
      "training loss 0.3315\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.1281%\n",
      "Epoch: 48\n",
      "training loss 0.3171\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.2512%\n",
      "Epoch: 49\n",
      "training loss 0.3035\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.1281%\n",
      "Epoch: 50\n",
      "training loss 0.2906\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.1281%\n",
      "Epoch: 51\n",
      "training loss 0.2783\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.1281%\n",
      "Epoch: 52\n",
      "training loss 0.2667\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.2512%\n",
      "Epoch: 53\n",
      "training loss 0.2557\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.4975%\n",
      "Epoch: 54\n",
      "training loss 0.2452\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 55\n",
      "training loss 0.2353\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 56\n",
      "training loss 0.2259\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.6207%\n",
      "Epoch: 57\n",
      "training loss 0.2169\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.6207%\n",
      "Epoch: 58\n",
      "training loss 0.2084\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.6207%\n",
      "Epoch: 59\n",
      "training loss 0.2002\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 60\n",
      "training loss 0.1925\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 61\n",
      "training loss 0.1852\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 62\n",
      "training loss 0.1782\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 63\n",
      "training loss 0.1715\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 64\n",
      "training loss 0.1652\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.8670%\n",
      "Epoch: 65\n",
      "training loss 0.1592\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.6207%\n",
      "Epoch: 66\n",
      "training loss 0.1535\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.4975%\n",
      "Epoch: 67\n",
      "training loss 0.1480\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3744%\n",
      "Epoch: 68\n",
      "training loss 0.1429\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.3744%\n",
      "Epoch: 69\n",
      "training loss 0.1379\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.4975%\n",
      "Epoch: 70\n",
      "training loss 0.1333\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.4975%\n",
      "Epoch: 71\n",
      "training loss 0.1288\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.6207%\n",
      "Epoch: 72\n",
      "training loss 0.1246\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 73\n",
      "training loss 0.1205\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 74\n",
      "training loss 0.1167\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.8670%\n",
      "Epoch: 75\n",
      "training loss 0.1130\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.8670%\n",
      "Epoch: 76\n",
      "training loss 0.1096\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.8670%\n",
      "Epoch: 77\n",
      "training loss 0.1063\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.8670%\n",
      "Epoch: 78\n",
      "training loss 0.1031\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.8670%\n",
      "Epoch: 79\n",
      "training loss 0.1001\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.8670%\n",
      "Epoch: 80\n",
      "training loss 0.0972\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.8670%\n",
      "Epoch: 81\n",
      "training loss 0.0945\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.8670%\n",
      "Epoch: 82\n",
      "training loss 0.0918\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.9901%\n",
      "Epoch: 83\n",
      "training loss 0.0893\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.9901%\n",
      "Epoch: 84\n",
      "training loss 0.0869\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.9901%\n",
      "Epoch: 85\n",
      "training loss 0.0846\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.8670%\n",
      "Epoch: 86\n",
      "training loss 0.0824\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 87\n",
      "training loss 0.0803\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 88\n",
      "training loss 0.0782\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 89\n",
      "training loss 0.0763\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 90\n",
      "training loss 0.0744\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.6207%\n",
      "Epoch: 91\n",
      "training loss 0.0726\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.6207%\n",
      "Epoch: 92\n",
      "training loss 0.0709\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 93\n",
      "training loss 0.0692\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 94\n",
      "training loss 0.0676\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 95\n",
      "training loss 0.0660\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.8670%\n",
      "Epoch: 96\n",
      "training loss 0.0645\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.8670%\n",
      "Epoch: 97\n",
      "training loss 0.0631\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 98\n",
      "training loss 0.0617\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n",
      "Epoch: 99\n",
      "training loss 0.0603\n",
      "Validtion: Average loss: 0.0000, Accuracy: 83.7438%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': [37.5615763546798,\n",
       "  39.77832512315271,\n",
       "  38.669950738916256,\n",
       "  38.793103448275865,\n",
       "  38.54679802955665,\n",
       "  40.14778325123153,\n",
       "  41.37931034482759,\n",
       "  42.73399014778325,\n",
       "  44.827586206896555,\n",
       "  47.53694581280788,\n",
       "  50.24630541871921,\n",
       "  53.20197044334975,\n",
       "  56.89655172413793,\n",
       "  58.86699507389162,\n",
       "  60.960591133004925,\n",
       "  63.05418719211823,\n",
       "  64.28571428571429,\n",
       "  67.36453201970443,\n",
       "  69.45812807881774,\n",
       "  70.93596059113301,\n",
       "  72.41379310344827,\n",
       "  73.39901477832512,\n",
       "  74.26108374384236,\n",
       "  74.8768472906404,\n",
       "  76.10837438423646,\n",
       "  76.9704433497537,\n",
       "  77.83251231527093,\n",
       "  78.32512315270937,\n",
       "  78.94088669950739,\n",
       "  79.43349753694581,\n",
       "  79.92610837438424,\n",
       "  80.91133004926108,\n",
       "  81.52709359605912,\n",
       "  81.77339901477832,\n",
       "  81.52709359605912,\n",
       "  82.26600985221675,\n",
       "  83.00492610837438,\n",
       "  82.75862068965517,\n",
       "  83.3743842364532,\n",
       "  83.12807881773399,\n",
       "  82.88177339901478,\n",
       "  83.00492610837438,\n",
       "  82.88177339901478,\n",
       "  83.2512315270936,\n",
       "  83.00492610837438,\n",
       "  83.00492610837438,\n",
       "  83.00492610837438,\n",
       "  83.12807881773399,\n",
       "  83.2512315270936,\n",
       "  83.12807881773399,\n",
       "  83.12807881773399,\n",
       "  83.12807881773399,\n",
       "  83.2512315270936,\n",
       "  83.49753694581281,\n",
       "  83.74384236453201,\n",
       "  83.74384236453201,\n",
       "  83.62068965517241,\n",
       "  83.62068965517241,\n",
       "  83.62068965517241,\n",
       "  83.74384236453201,\n",
       "  83.74384236453201,\n",
       "  83.74384236453201,\n",
       "  83.74384236453201,\n",
       "  83.74384236453201,\n",
       "  83.86699507389163,\n",
       "  83.62068965517241,\n",
       "  83.49753694581281,\n",
       "  83.3743842364532,\n",
       "  83.3743842364532,\n",
       "  83.49753694581281,\n",
       "  83.49753694581281,\n",
       "  83.62068965517241,\n",
       "  83.74384236453201,\n",
       "  83.74384236453201,\n",
       "  83.86699507389163,\n",
       "  83.86699507389163,\n",
       "  83.86699507389163,\n",
       "  83.86699507389163,\n",
       "  83.86699507389163,\n",
       "  83.86699507389163,\n",
       "  83.86699507389163,\n",
       "  83.86699507389163,\n",
       "  83.99014778325123,\n",
       "  83.99014778325123,\n",
       "  83.99014778325123,\n",
       "  83.86699507389163,\n",
       "  83.74384236453201,\n",
       "  83.74384236453201,\n",
       "  83.74384236453201,\n",
       "  83.74384236453201,\n",
       "  83.62068965517241,\n",
       "  83.62068965517241,\n",
       "  83.74384236453201,\n",
       "  83.74384236453201,\n",
       "  83.74384236453201,\n",
       "  83.86699507389163,\n",
       "  83.86699507389163,\n",
       "  83.74384236453201,\n",
       "  83.74384236453201,\n",
       "  83.74384236453201]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
