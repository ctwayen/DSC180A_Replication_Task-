{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from numpy.random import choice\n",
    "from torch.nn.parameter import Parameter\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(A, X, len_walk, num_neigh, agg_func):\n",
    "    norm = torch.div(A, torch.sum(A, axis=1))\n",
    "    norm = torch.matrix_power(norm, len_walk)\n",
    "    result = torch.zeros(X.shape)\n",
    "    for i in range(A.shape[0]):\n",
    "        x = A[i].cpu().detach().numpy()\n",
    "        ind = np.random.choice(range(x.shape[0]), num_neigh, replace=False)\n",
    "        if agg_func == \"MEAN\":\n",
    "            result[i] = torch.mean(X[ind], axis=0)\n",
    "        else:\n",
    "            result[i] = torch.max(X[ind], axis=0).values\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SageLayer(nn.Module):\n",
    "    def __init__(self, F, O, len_walk = 2, num_neigh = 10, agg_func=\"MEAN\", bias=True): \n",
    "        super(SageLayer, self).__init__()\n",
    "        self.F = F\n",
    "        self.O = O\n",
    "        self.weight = Parameter(torch.FloatTensor(2 * F, O))\n",
    "        self.len_walk = len_walk\n",
    "        self.num_neigh = num_neigh\n",
    "        self.agg_func = agg_func\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(O))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, X, A):\n",
    "        aggregated = aggregate(A, X, self.len_walk, self.num_neigh, self.agg_func)\n",
    "        aggregated = aggregated.to(X.device)\n",
    "        combined = torch.cat([X, aggregated], dim=1)\n",
    "        combined = torch.mm(combined, self.weight)\n",
    "        if self.bias is not None:\n",
    "            return combined + self.bias\n",
    "        else:\n",
    "            return combined\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSage_model(nn.Module):\n",
    "    def __init__(self, X, A, n=0, F=1433, class_num=7, \n",
    "                 agg_func='MEAN', hidden_neuron=200, len_walk=2, num_neigh=10, bias=True):\n",
    "        super(GraphSage_model, self).__init__()\n",
    "\n",
    "        self.F = F\n",
    "        self.class_num = class_num\n",
    "        self.n = n\n",
    "        self.agg_func = agg_func\n",
    "        self.X = X\n",
    "        self.A = A\n",
    "        \n",
    "        self.gs1 = SageLayer(F, hidden_neuron, len_walk=len_walk, num_neigh=num_neigh, agg_func=agg_func, bias=bias)\n",
    "        self.gsh = SageLayer(hidden_neuron, hidden_neuron, len_walk=len_walk, num_neigh=num_neigh, agg_func=agg_func, bias=bias)\n",
    "        self.gs2 = SageLayer(hidden_neuron, self.class_num, len_walk=len_walk, num_neigh=num_neigh, agg_func=agg_func, bias=bias)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.gs1(X, self.A)\n",
    "        X = F.relu(X)\n",
    "        for i in range(self.n):\n",
    "            X = self.gsh(X, self.A)\n",
    "            X = F.relu(X)\n",
    "        X = self.gs2(X, self.A)\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSage():\n",
    "    def __init__(self, A, X, y, device='cuda', n=0, F=1433, class_num=7, agg_func=\"MEAN\", hidden_neuron=200,\n",
    "                len_walk=2, num_neigh=10, bias=True, val_size=0.3):\n",
    "        if device == 'cuda':\n",
    "            self.device= torch.device('cuda')\n",
    "        else:\n",
    "            assert('only support cuda')\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(y)\n",
    "        y = le.transform(y)\n",
    "        \n",
    "        X = torch.tensor(X)\n",
    "        X = X.type(torch.float)\n",
    "        y = torch.tensor(y)\n",
    "        y = y.type(torch.long)\n",
    "        y = y.to(self.device)\n",
    "        A = torch.from_numpy(A).float()\n",
    "        \n",
    "        self.X = X.to(self.device)\n",
    "        self.A = A.to(self.device)\n",
    "        self.y = y.to(self.device)\n",
    "        \n",
    "        train_idx = np.random.choice(self.X.shape[0], round(self.X.shape[0]*(1-val_size)), replace=False)\n",
    "        val_idx = np.array([x for x in range(X.shape[0]) if x not in train_idx])\n",
    "        print(\"Train length :{a}, Validation length :{b}\".format(a=len(train_idx), b=len(val_idx)))\n",
    "        \n",
    "        self.idx_train = torch.LongTensor(train_idx)\n",
    "        self.idx_val = torch.LongTensor(val_idx)\n",
    "        self.graphsage = GraphSage_model(self.X, self.A, n=n, F=F, agg_func=agg_func, hidden_neuron=hidden_neuron,\n",
    "                                         class_num = class_num, len_walk=len_walk, bias=bias, num_neigh=num_neigh)\n",
    "        self.graphsage.to(self.device)\n",
    "        \n",
    "    def train(self, optimizer, epoch):\n",
    "        self.graphsage.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = self.graphsage(self.X)\n",
    "        loss = F.cross_entropy(output[self.idx_train], self.y[self.idx_train])\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        print('Epoch: {x}'.format(x=epoch))\n",
    "        print('training loss {:.4f}'.format(loss.item()))\n",
    "            \n",
    "    def test(self):\n",
    "        self.graphsage.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            output = self.graphsage(self.X)\n",
    "            test_loss = F.cross_entropy(output[self.idx_val], self.y[self.idx_val], reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)[self.idx_val]\n",
    "            correct += pred.eq(self.y[self.idx_val].view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(self.idx_val)\n",
    "        print('Validtion: Average loss: {:.4f}, Accuracy: {:.4f}%'.format(test_loss, 100. * correct / len(self.idx_val)))\n",
    "        return 100. * correct / len(self.idx_val)\n",
    "    \n",
    "    def train_epoch(self, epochs=50, lr=1e-3):\n",
    "        acc = []\n",
    "        optimizer = optim.Adam(self.graphsage.parameters(), lr=lr)#, weight_decay=1e-1)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train(optimizer, epoch)\n",
    "            accs = self.test()\n",
    "            acc.append(accs)\n",
    "        accs = {'acc': acc}\n",
    "        return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coraloader import cora_loader\n",
    "cora = cora_loader('data' + '/cora.content', 'data' + '/cora.cites', None)\n",
    "X, y, A = cora.get_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length :1896, Validation length :812\n"
     ]
    }
   ],
   "source": [
    "model = GraphSage(A, X, y, agg_func=\"MAX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "training loss 2.1360\n",
      "Validtion: Average loss: 1.8881, Accuracy: 25.3695%\n",
      "Epoch: 1\n",
      "training loss 1.8759\n",
      "Validtion: Average loss: 1.8104, Accuracy: 30.2956%\n",
      "Epoch: 2\n",
      "training loss 1.7535\n",
      "Validtion: Average loss: 1.7967, Accuracy: 29.8030%\n",
      "Epoch: 3\n",
      "training loss 1.6908\n",
      "Validtion: Average loss: 1.7592, Accuracy: 30.2956%\n",
      "Epoch: 4\n",
      "training loss 1.6239\n",
      "Validtion: Average loss: 1.6924, Accuracy: 33.2512%\n",
      "Epoch: 5\n",
      "training loss 1.5471\n",
      "Validtion: Average loss: 1.6409, Accuracy: 38.3005%\n",
      "Epoch: 6\n",
      "training loss 1.4714\n",
      "Validtion: Average loss: 1.6047, Accuracy: 44.0887%\n",
      "Epoch: 7\n",
      "training loss 1.4066\n",
      "Validtion: Average loss: 1.5485, Accuracy: 47.9064%\n",
      "Epoch: 8\n",
      "training loss 1.3351\n",
      "Validtion: Average loss: 1.4947, Accuracy: 50.3695%\n",
      "Epoch: 9\n",
      "training loss 1.2567\n",
      "Validtion: Average loss: 1.4508, Accuracy: 54.0640%\n",
      "Epoch: 10\n",
      "training loss 1.1881\n",
      "Validtion: Average loss: 1.3943, Accuracy: 54.6798%\n",
      "Epoch: 11\n",
      "training loss 1.1179\n",
      "Validtion: Average loss: 1.3464, Accuracy: 55.2956%\n",
      "Epoch: 12\n",
      "training loss 1.0454\n",
      "Validtion: Average loss: 1.3091, Accuracy: 55.7882%\n",
      "Epoch: 13\n",
      "training loss 0.9814\n",
      "Validtion: Average loss: 1.2632, Accuracy: 58.8670%\n",
      "Epoch: 14\n",
      "training loss 0.9276\n",
      "Validtion: Average loss: 1.2139, Accuracy: 60.5911%\n",
      "Epoch: 15\n",
      "training loss 0.8655\n",
      "Validtion: Average loss: 1.1672, Accuracy: 62.5616%\n",
      "Epoch: 16\n",
      "training loss 0.8109\n",
      "Validtion: Average loss: 1.1315, Accuracy: 64.9015%\n",
      "Epoch: 17\n",
      "training loss 0.7624\n",
      "Validtion: Average loss: 1.0890, Accuracy: 66.3793%\n",
      "Epoch: 18\n",
      "training loss 0.7184\n",
      "Validtion: Average loss: 1.0552, Accuracy: 68.5961%\n",
      "Epoch: 19\n",
      "training loss 0.6689\n",
      "Validtion: Average loss: 1.0290, Accuracy: 69.0887%\n",
      "Epoch: 20\n",
      "training loss 0.6332\n",
      "Validtion: Average loss: 0.9903, Accuracy: 70.4433%\n",
      "Epoch: 21\n",
      "training loss 0.5878\n",
      "Validtion: Average loss: 0.9727, Accuracy: 69.4581%\n",
      "Epoch: 22\n",
      "training loss 0.5492\n",
      "Validtion: Average loss: 0.9431, Accuracy: 71.1823%\n",
      "Epoch: 23\n",
      "training loss 0.5190\n",
      "Validtion: Average loss: 0.9322, Accuracy: 70.6897%\n",
      "Epoch: 24\n",
      "training loss 0.4815\n",
      "Validtion: Average loss: 0.9030, Accuracy: 71.1823%\n",
      "Epoch: 25\n",
      "training loss 0.4558\n",
      "Validtion: Average loss: 0.9011, Accuracy: 71.5517%\n",
      "Epoch: 26\n",
      "training loss 0.4333\n",
      "Validtion: Average loss: 0.8718, Accuracy: 73.5222%\n",
      "Epoch: 27\n",
      "training loss 0.4046\n",
      "Validtion: Average loss: 0.8596, Accuracy: 71.1823%\n",
      "Epoch: 28\n",
      "training loss 0.3854\n",
      "Validtion: Average loss: 0.8523, Accuracy: 72.7833%\n",
      "Epoch: 29\n",
      "training loss 0.3675\n",
      "Validtion: Average loss: 0.8357, Accuracy: 73.6453%\n",
      "Epoch: 30\n",
      "training loss 0.3484\n",
      "Validtion: Average loss: 0.8245, Accuracy: 73.3990%\n",
      "Epoch: 31\n",
      "training loss 0.3248\n",
      "Validtion: Average loss: 0.8263, Accuracy: 73.7685%\n",
      "Epoch: 32\n",
      "training loss 0.3088\n",
      "Validtion: Average loss: 0.8191, Accuracy: 74.0148%\n",
      "Epoch: 33\n",
      "training loss 0.2948\n",
      "Validtion: Average loss: 0.8051, Accuracy: 73.1527%\n",
      "Epoch: 34\n",
      "training loss 0.2783\n",
      "Validtion: Average loss: 0.8089, Accuracy: 74.3842%\n",
      "Epoch: 35\n",
      "training loss 0.2641\n",
      "Validtion: Average loss: 0.7981, Accuracy: 73.6453%\n",
      "Epoch: 36\n",
      "training loss 0.2528\n",
      "Validtion: Average loss: 0.7870, Accuracy: 74.1379%\n",
      "Epoch: 37\n",
      "training loss 0.2393\n",
      "Validtion: Average loss: 0.7848, Accuracy: 74.3842%\n",
      "Epoch: 38\n",
      "training loss 0.2267\n",
      "Validtion: Average loss: 0.7888, Accuracy: 74.1379%\n",
      "Epoch: 39\n",
      "training loss 0.2179\n",
      "Validtion: Average loss: 0.7842, Accuracy: 74.5074%\n",
      "Epoch: 40\n",
      "training loss 0.2067\n",
      "Validtion: Average loss: 0.7729, Accuracy: 75.2463%\n",
      "Epoch: 41\n",
      "training loss 0.1979\n",
      "Validtion: Average loss: 0.7759, Accuracy: 74.7537%\n",
      "Epoch: 42\n",
      "training loss 0.1895\n",
      "Validtion: Average loss: 0.7790, Accuracy: 73.2759%\n",
      "Epoch: 43\n",
      "training loss 0.1825\n",
      "Validtion: Average loss: 0.7823, Accuracy: 74.7537%\n",
      "Epoch: 44\n",
      "training loss 0.1738\n",
      "Validtion: Average loss: 0.7862, Accuracy: 74.0148%\n",
      "Epoch: 45\n",
      "training loss 0.1676\n",
      "Validtion: Average loss: 0.7810, Accuracy: 73.6453%\n",
      "Epoch: 46\n",
      "training loss 0.1599\n",
      "Validtion: Average loss: 0.7800, Accuracy: 74.0148%\n",
      "Epoch: 47\n",
      "training loss 0.1528\n",
      "Validtion: Average loss: 0.7842, Accuracy: 74.5074%\n",
      "Epoch: 48\n",
      "training loss 0.1460\n",
      "Validtion: Average loss: 0.7862, Accuracy: 74.2611%\n",
      "Epoch: 49\n",
      "training loss 0.1407\n",
      "Validtion: Average loss: 0.7837, Accuracy: 73.6453%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': [25.36945812807882,\n",
       "  30.295566502463053,\n",
       "  29.80295566502463,\n",
       "  30.295566502463053,\n",
       "  33.251231527093594,\n",
       "  38.30049261083744,\n",
       "  44.08866995073892,\n",
       "  47.9064039408867,\n",
       "  50.36945812807882,\n",
       "  54.064039408866996,\n",
       "  54.679802955665025,\n",
       "  55.29556650246305,\n",
       "  55.78817733990148,\n",
       "  58.86699507389162,\n",
       "  60.59113300492611,\n",
       "  62.5615763546798,\n",
       "  64.90147783251231,\n",
       "  66.37931034482759,\n",
       "  68.5960591133005,\n",
       "  69.08866995073892,\n",
       "  70.44334975369458,\n",
       "  69.45812807881774,\n",
       "  71.18226600985221,\n",
       "  70.6896551724138,\n",
       "  71.18226600985221,\n",
       "  71.55172413793103,\n",
       "  73.52216748768473,\n",
       "  71.18226600985221,\n",
       "  72.78325123152709,\n",
       "  73.64532019704434,\n",
       "  73.39901477832512,\n",
       "  73.76847290640394,\n",
       "  74.01477832512315,\n",
       "  73.15270935960591,\n",
       "  74.38423645320196,\n",
       "  73.64532019704434,\n",
       "  74.13793103448276,\n",
       "  74.38423645320196,\n",
       "  74.13793103448276,\n",
       "  74.50738916256158,\n",
       "  75.24630541871922,\n",
       "  74.75369458128078,\n",
       "  73.27586206896552,\n",
       "  74.75369458128078,\n",
       "  74.01477832512315,\n",
       "  73.64532019704434,\n",
       "  74.01477832512315,\n",
       "  74.50738916256158,\n",
       "  74.26108374384236,\n",
       "  73.64532019704434]}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
