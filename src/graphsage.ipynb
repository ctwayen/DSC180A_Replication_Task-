{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edited based on https://github.com/williamleif/GraphSAGE/blob/master/graphsage/aggregators.py\n",
    "import sys, os\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size, num_classes):\n",
    "        super(Classification, self).__init__()\n",
    "\n",
    "        #self.weight = nn.Parameter(torch.FloatTensor(emb_size, num_classes))\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(emb_size, num_classes)\n",
    "        )\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            if len(param.size()) == 2:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, embeds):\n",
    "        logists = torch.log_softmax(self.layer(embeds), 1)\n",
    "        return logists\n",
    "\n",
    "class UnsupervisedLoss(object):\n",
    "    def __init__(self, adj_lists, train_nodes, device):\n",
    "        super(UnsupervisedLoss, self).__init__()\n",
    "        self.Q = 10\n",
    "        self.N_WALKS = 6\n",
    "        self.WALK_LEN = 1\n",
    "        self.N_WALK_LEN = 5\n",
    "        self.MARGIN = 3\n",
    "        self.adj_lists = adj_lists\n",
    "        self.train_nodes = train_nodes\n",
    "        self.device = device\n",
    "\n",
    "        self.target_nodes = None\n",
    "        self.positive_pairs = []\n",
    "        self.negtive_pairs = []\n",
    "        self.node_positive_pairs = {}\n",
    "        self.node_negtive_pairs = {}\n",
    "        self.unique_nodes_batch = []\n",
    "\n",
    "    def get_loss_sage(self, embeddings, nodes):\n",
    "        assert len(embeddings) == len(self.unique_nodes_batch)\n",
    "        assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n",
    "        node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n",
    "\n",
    "        nodes_score = []\n",
    "        assert len(self.node_positive_pairs) == len(self.node_negtive_pairs)\n",
    "        for node in self.node_positive_pairs:\n",
    "            pps = self.node_positive_pairs[node]\n",
    "            nps = self.node_negtive_pairs[node]\n",
    "            if len(pps) == 0 or len(nps) == 0:\n",
    "                continue\n",
    "\n",
    "            # Q * Exception(negative score)\n",
    "            indexs = [list(x) for x in zip(*nps)]\n",
    "            node_indexs = [node2index[x] for x in indexs[0]]\n",
    "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "            neg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "            neg_score = self.Q*torch.mean(torch.log(torch.sigmoid(-neg_score)), 0)\n",
    "            #print(neg_score)\n",
    "\n",
    "            # multiple positive score\n",
    "            indexs = [list(x) for x in zip(*pps)]\n",
    "            node_indexs = [node2index[x] for x in indexs[0]]\n",
    "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "            pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "            pos_score = torch.log(torch.sigmoid(pos_score))\n",
    "            #print(pos_score)\n",
    "\n",
    "            nodes_score.append(torch.mean(- pos_score - neg_score).view(1,-1))\n",
    "        loss = torch.mean(torch.cat(nodes_score, 0))\n",
    "        return loss\n",
    "\n",
    "    def get_loss_margin(self, embeddings, nodes):\n",
    "        assert len(embeddings) == len(self.unique_nodes_batch)\n",
    "        assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n",
    "        node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n",
    "\n",
    "        nodes_score = []\n",
    "        assert len(self.node_positive_pairs) == len(self.node_negtive_pairs)\n",
    "        for node in self.node_positive_pairs:\n",
    "            pps = self.node_positive_pairs[node]\n",
    "            nps = self.node_negtive_pairs[node]\n",
    "            if len(pps) == 0 or len(nps) == 0:\n",
    "                continue\n",
    "\n",
    "            indexs = [list(x) for x in zip(*pps)]\n",
    "            node_indexs = [node2index[x] for x in indexs[0]]\n",
    "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "            pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "            pos_score, _ = torch.min(torch.log(torch.sigmoid(pos_score)), 0)\n",
    "\n",
    "            indexs = [list(x) for x in zip(*nps)]\n",
    "            node_indexs = [node2index[x] for x in indexs[0]]\n",
    "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "            neg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "            neg_score, _ = torch.max(torch.log(torch.sigmoid(neg_score)), 0)\n",
    "\n",
    "            nodes_score.append(torch.max(torch.tensor(0.0).to(self.device), neg_score-pos_score+self.MARGIN).view(1,-1))\n",
    "            # nodes_score.append((-pos_score - neg_score).view(1,-1))\n",
    "\n",
    "        loss = torch.mean(torch.cat(nodes_score, 0),0)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def extend_nodes(self, nodes, num_neg=6):\n",
    "        self.positive_pairs = []\n",
    "        self.node_positive_pairs = {}\n",
    "        self.negtive_pairs = []\n",
    "        self.node_negtive_pairs = {}\n",
    "\n",
    "        self.target_nodes = nodes\n",
    "        self.get_positive_nodes(nodes)\n",
    "        # print(self.positive_pairs)\n",
    "        self.get_negtive_nodes(nodes, num_neg)\n",
    "        # print(self.negtive_pairs)\n",
    "        self.unique_nodes_batch = list(set([i for x in self.positive_pairs for i in x]) | set([i for x in self.negtive_pairs for i in x]))\n",
    "        assert set(self.target_nodes) < set(self.unique_nodes_batch)\n",
    "        return self.unique_nodes_batch\n",
    "\n",
    "    def get_positive_nodes(self, nodes):\n",
    "        return self._run_random_walks(nodes)\n",
    "\n",
    "    def get_negtive_nodes(self, nodes, num_neg):\n",
    "        for node in nodes:\n",
    "            neighbors = set([node])\n",
    "            frontier = set([node])\n",
    "            for i in range(self.N_WALK_LEN):\n",
    "                current = set()\n",
    "                for outer in frontier:\n",
    "                    current |= self.adj_lists[int(outer)]\n",
    "                frontier = current - neighbors\n",
    "                neighbors |= current\n",
    "            far_nodes = set(self.train_nodes) - neighbors\n",
    "            neg_samples = random.sample(far_nodes, num_neg) if num_neg < len(far_nodes) else far_nodes\n",
    "            self.negtive_pairs.extend([(node, neg_node) for neg_node in neg_samples])\n",
    "            self.node_negtive_pairs[node] = [(node, neg_node) for neg_node in neg_samples]\n",
    "        return self.negtive_pairs\n",
    "\n",
    "    def _run_random_walks(self, nodes):\n",
    "        for node in nodes:\n",
    "            if len(self.adj_lists[int(node)]) == 0:\n",
    "                continue\n",
    "            cur_pairs = []\n",
    "            for i in range(self.N_WALKS):\n",
    "                curr_node = node\n",
    "                for j in range(self.WALK_LEN):\n",
    "                    neighs = self.adj_lists[int(curr_node)]\n",
    "                    next_node = random.choice(list(neighs))\n",
    "                    # self co-occurrences are useless\n",
    "                    if next_node != node and next_node in self.train_nodes:\n",
    "                        self.positive_pairs.append((node,next_node))\n",
    "                        cur_pairs.append((node,next_node))\n",
    "                curr_node = next_node\n",
    "\n",
    "            self.node_positive_pairs[node] = cur_pairs\n",
    "        return self.positive_pairs\n",
    "\n",
    "class SageLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes a node's using 'convolutional' GraphSage approach\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, out_size, gcn=False): \n",
    "        super(SageLayer, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "\n",
    "        self.gcn = gcn\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_size, self.input_size if self.gcn else 2 * self.input_size))\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, self_feats, aggregate_feats, neighs=None):\n",
    "        \"\"\"\n",
    "        Generates embeddings for a batch of nodes.\n",
    "        nodes\t -- list of nodes\n",
    "        \"\"\"\n",
    "        if not self.gcn:\n",
    "            combined = torch.cat([self_feats, aggregate_feats], dim=1)\n",
    "        else:\n",
    "            combined = aggregate_feats\n",
    "        combined = F.relu(self.weight.mm(combined.t())).t()\n",
    "        return combined\n",
    "\n",
    "class GraphSage(nn.Module):\n",
    "    \"\"\"docstring for GraphSage\"\"\"\n",
    "    def __init__(self, num_layers, input_size, out_size, raw_features, adj_lists, device, gcn=False, agg_func='MEAN'):\n",
    "        super(GraphSage, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.out_size = out_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gcn = gcn\n",
    "        self.device = device\n",
    "        self.agg_func = agg_func\n",
    "\n",
    "        self.raw_features = raw_features\n",
    "        self.adj_lists = adj_lists\n",
    "\n",
    "        for index in range(1, num_layers+1):\n",
    "            layer_size = out_size if index != 1 else input_size\n",
    "            setattr(self, 'sage_layer'+str(index), SageLayer(layer_size, out_size, gcn=self.gcn))\n",
    "\n",
    "    def forward(self, nodes_batch):\n",
    "        \"\"\"\n",
    "        Generates embeddings for a batch of nodes.\n",
    "        nodes_batch\t-- batch of nodes to learn the embeddings\n",
    "        \"\"\"\n",
    "        lower_layer_nodes = list(nodes_batch)\n",
    "        nodes_batch_layers = [(lower_layer_nodes,)]\n",
    "        # self.dc.logger.info('get_unique_neighs.')\n",
    "        for i in range(self.num_layers):\n",
    "            lower_samp_neighs, lower_layer_nodes_dict, lower_layer_nodes= self._get_unique_neighs_list(lower_layer_nodes)\n",
    "            nodes_batch_layers.insert(0, (lower_layer_nodes, lower_samp_neighs, lower_layer_nodes_dict))\n",
    "\n",
    "        assert len(nodes_batch_layers) == self.num_layers + 1\n",
    "\n",
    "        pre_hidden_embs = self.raw_features\n",
    "        for index in range(1, self.num_layers+1):\n",
    "            nb = nodes_batch_layers[index][0]\n",
    "            pre_neighs = nodes_batch_layers[index-1]\n",
    "            # self.dc.logger.info('aggregate_feats.')\n",
    "            aggregate_feats = self.aggregate(nb, pre_hidden_embs, pre_neighs)\n",
    "            sage_layer = getattr(self, 'sage_layer'+str(index))\n",
    "            if index > 1:\n",
    "                nb = self._nodes_map(nb, pre_hidden_embs, pre_neighs)\n",
    "            # self.dc.logger.info('sage_layer.')\n",
    "            cur_hidden_embs = sage_layer(self_feats=pre_hidden_embs[nb],\n",
    "                                        aggregate_feats=aggregate_feats)\n",
    "            pre_hidden_embs = cur_hidden_embs\n",
    "\n",
    "        return pre_hidden_embs\n",
    "\n",
    "    def _nodes_map(self, nodes, hidden_embs, neighs):\n",
    "        layer_nodes, samp_neighs, layer_nodes_dict = neighs\n",
    "        assert len(samp_neighs) == len(nodes)\n",
    "        index = [layer_nodes_dict[x] for x in nodes]\n",
    "        return index\n",
    "\n",
    "    def _get_unique_neighs_list(self, nodes, num_sample=10):\n",
    "        _set = set\n",
    "        to_neighs = [self.adj_lists[int(node)] for node in nodes]\n",
    "        if not num_sample is None:\n",
    "            _sample = random.sample\n",
    "            samp_neighs = [_set(_sample(to_neigh, num_sample)) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n",
    "        else:\n",
    "            samp_neighs = to_neighs\n",
    "        samp_neighs = [samp_neigh | set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]\n",
    "        _unique_nodes_list = list(set.union(*samp_neighs))\n",
    "        i = list(range(len(_unique_nodes_list)))\n",
    "        unique_nodes = dict(list(zip(_unique_nodes_list, i)))\n",
    "        return samp_neighs, unique_nodes, _unique_nodes_list\n",
    "\n",
    "\tdef aggregate(self, nodes, pre_hidden_embs, pre_neighs, num_sample=10):\n",
    "\t\tunique_nodes_list, samp_neighs, unique_nodes = pre_neighs\n",
    "\n",
    "\t\tassert len(nodes) == len(samp_neighs)\n",
    "\t\tindicator = [(nodes[i] in samp_neighs[i]) for i in range(len(samp_neighs))]\n",
    "\t\tassert (False not in indicator)\n",
    "\t\tif not self.gcn:\n",
    "\t\t\tsamp_neighs = [(samp_neighs[i]-set([nodes[i]])) for i in range(len(samp_neighs))]\n",
    "\t\t# self.dc.logger.info('2')\n",
    "\t\tif len(pre_hidden_embs) == len(unique_nodes):\n",
    "\t\t\tembed_matrix = pre_hidden_embs\n",
    "\t\telse:\n",
    "\t\t\tembed_matrix = pre_hidden_embs[torch.LongTensor(unique_nodes_list)]\n",
    "\t\t# self.dc.logger.info('3')\n",
    "\t\tmask = torch.zeros(len(samp_neighs), len(unique_nodes))\n",
    "\t\tcolumn_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n",
    "\t\trow_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "\t\tmask[row_indices, column_indices] = 1\n",
    "\t\t# self.dc.logger.info('4')\n",
    "\n",
    "\t\tif self.agg_func == 'MEAN':\n",
    "\t\t\tnum_neigh = mask.sum(1, keepdim=True)\n",
    "\t\t\tmask = mask.div(num_neigh).to(embed_matrix.device)\n",
    "\t\t\taggregate_feats = mask.mm(embed_matrix)\n",
    "\n",
    "\t\telif self.agg_func == 'MAX':\n",
    "\t\t\t# print(mask)\n",
    "\t\t\tindexs = [x.nonzero() for x in mask==1]\n",
    "\t\t\taggregate_feats = []\n",
    "\t\t\t# self.dc.logger.info('5')\n",
    "\t\t\tfor feat in [embed_matrix[x.squeeze()] for x in indexs]:\n",
    "\t\t\t\tif len(feat.size()) == 1:\n",
    "\t\t\t\t\taggregate_feats.append(feat.view(1, -1))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\taggregate_feats.append(torch.max(feat,0)[0].view(1, -1))\n",
    "\t\t\taggregate_feats = torch.cat(aggregate_feats, 0)\n",
    "\n",
    "\t\t# self.dc.logger.info('6')\n",
    "\t\t\n",
    "\t\treturn aggregate_feats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
